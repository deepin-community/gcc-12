From 60d6f7d1fbbf0d09b5b039873ce7f13ad90a5c09 Mon Sep 17 00:00:00 2001
From: Guo Jie <guojie@loongson.cn>
Date: Thu, 28 Nov 2024 08:19:05 +0000
Subject: [PATCH 126/301] LoongArch: Optimizations of vector construction.

 gcc/ChangeLog:

   * config/loongarch/lasx.md (lasx_vecinit_merge_<LASX:mode>): New
   pattern for vector construction.
   (vec_set<mode>_internal): Ditto.
   (lasx_xvinsgr2vr_<mode256_i_half>_internal): Ditto.
   (lasx_xvilvl_<lasxfmt_f>_internal): Ditto.
   * config/loongarch/loongarch.cc (loongarch_expand_vector_init):
   Optimized the implementation of vector construction.
   (loongarch_expand_vector_init_same): New function.
   * config/loongarch/lsx.md (lsx_vilvl_<lsxfmt_f>_internal): New
   pattern for vector construction.
   (lsx_vreplvei_mirror_<lsxfmt_f>): New pattern for vector
   construction.
   (vec_concatv2df): Ditto.
   (vec_concatv4sf): Ditto.
---
 .../config/loongarch/genopts/loongarch.opt.in |   26 +-
 src/gcc/config/loongarch/gnu-user.h           |    1 +
 src/gcc/config/loongarch/lasx.md              |   69 +
 src/gcc/config/loongarch/loongarch-opts.cc    |    4 +-
 src/gcc/config/loongarch/loongarch-opts.h     |   38 +-
 src/gcc/config/loongarch/loongarch.cc         | 3062 ++++++++---------
 src/gcc/config/loongarch/loongarch.h          |    5 +
 src/gcc/config/loongarch/loongarch.opt        |   26 +-
 src/gcc/config/loongarch/lsx.md               |  134 +
 src/gcc/config/loongarch/sync.md              |   46 +-
 10 files changed, 1782 insertions(+), 1629 deletions(-)

diff --git a/src/gcc/config/loongarch/genopts/loongarch.opt.in b/src/gcc/config/loongarch/genopts/loongarch.opt.in
index 5e0945f22..f18733c24 100644
--- a/src/gcc/config/loongarch/genopts/loongarch.opt.in
+++ b/src/gcc/config/loongarch/genopts/loongarch.opt.in
@@ -83,7 +83,7 @@ Enum(isa_ext_simd) String(@@STR_ISA_EXT_LASX@@) Value(ISA_EXT_SIMD_LASX)
 
 m@@OPTSTR_ISA_EXT_SIMD@@=
 Target RejectNegative Joined ToLower Enum(isa_ext_simd) Var(la_opt_simd) Init(M_OPT_UNSET)
--m@@OPTSTR_ISA_EXT_SIMD@@=SIMD Generate code for the given SIMD extension.
+-m@@OPTSTR_ISA_EXT_SIMD@@=SIMD	Generate code for the given SIMD extension.
 
 m@@STR_ISA_EXT_LSX@@
 Target Driver Defer Var(la_deferred_options)
@@ -143,13 +143,13 @@ m@@OPTSTR_ABI_BASE@@=
 Target RejectNegative Joined ToLower Enum(abi_base) Var(la_opt_abi_base) Init(M_OPT_UNSET)
 -m@@OPTSTR_ABI_BASE@@=BASEABI	Generate code that conforms to the given BASEABI.
 
+
 ;; ABI Extension
 Variable
 int la_opt_abi_ext = M_OPT_UNSET
 
-
 mbranch-cost=
-Target RejectNegative Joined UInteger Var(loongarch_branch_cost) Save
+Target RejectNegative Joined UInteger Var(loongarch_branch_cost)
 -mbranch-cost=COST	Set the cost of branches to roughly COST instructions.
 
 mmemvec-cost=
@@ -157,32 +157,32 @@ Target RejectNegative Joined UInteger Var(loongarch_vector_access_cost) IntegerR
 mmemvec-cost=COST      Set the cost of vector memory access instructions.
 
 mcheck-zero-division
-Target Mask(CHECK_ZERO_DIV) Save
+Target Mask(CHECK_ZERO_DIV)
 Trap on integer divide by zero.
 
 mcond-move-int
-Target Var(TARGET_COND_MOVE_INT) Init(1) Save
+Target Var(TARGET_COND_MOVE_INT) Init(1)
 Conditional moves for integral are enabled.
 
 mcond-move-float
-Target Var(TARGET_COND_MOVE_FLOAT) Init(1) Save
+Target Var(TARGET_COND_MOVE_FLOAT) Init(1)
 Conditional moves for float are enabled.
 
 mmemcpy
-Target Mask(MEMCPY) Save
+Target Mask(MEMCPY)
 Prevent optimizing block moves, which is also the default behavior of -Os.
 
 mstrict-align
-Target Var(TARGET_STRICT_ALIGN) Init(0) Save
+Target Var(TARGET_STRICT_ALIGN) Init(0)
 Do not generate unaligned memory accesses.
 
 mmax-inline-memcpy-size=
-Target Joined RejectNegative UInteger Var(loongarch_max_inline_memcpy_size) Init(1024) Save
+Target Joined RejectNegative UInteger Var(loongarch_max_inline_memcpy_size) Init(1024)
 -mmax-inline-memcpy-size=SIZE	Set the max size of memcpy to inline, default is 1024.
 
 mexplicit-relocs
 Target Var(TARGET_EXPLICIT_RELOCS) Init(HAVE_AS_EXPLICIT_RELOCS & !HAVE_AS_MRELAX_OPTION)
-Use %reloc() assembly operators
+Use %reloc() assembly operators.
 
 ; The code model option names for -mcmodel.
 Enum
@@ -216,10 +216,6 @@ Target Var(TARGET_DIRECT_EXTERN_ACCESS) Init(0)
 Avoid using the GOT to access external symbols.
 
 mrelax
-Target Var(loongarch_mrelax) Init(HAVE_AS_MRELAX_OPTION && HAVE_AS_COND_BRANCH_RELAXATION)
+Target Var(loongarch_mrelax) Init(HAVE_AS_MRELAX_OPTION)
 Take advantage of linker relaxations to reduce the number of instructions
 required to materialize symbol addresses.
-
-mpass-mrelax-to-as
-Target Var(loongarch_pass_mrelax_to_as) Init(HAVE_AS_MRELAX_OPTION)
-Pass -mrelax or -mno-relax option to the assembler.
diff --git a/src/gcc/config/loongarch/gnu-user.h b/src/gcc/config/loongarch/gnu-user.h
index 5b6a010d5..9fc49dc8f 100644
--- a/src/gcc/config/loongarch/gnu-user.h
+++ b/src/gcc/config/loongarch/gnu-user.h
@@ -51,6 +51,7 @@ along with GCC; see the file COPYING3.  If not see
   "%{static-pie: -static -pie --no-dynamic-linker -z text}}" \
   "%{mno-relax: --no-relax}"
 
+
 /* Similar to standard Linux, but adding -ffast-math support.  */
 #undef GNU_USER_TARGET_MATHFILE_SPEC
 #define GNU_USER_TARGET_MATHFILE_SPEC \
diff --git a/src/gcc/config/loongarch/lasx.md b/src/gcc/config/loongarch/lasx.md
index 8111c8bb7..2bc5d47ed 100644
--- a/src/gcc/config/loongarch/lasx.md
+++ b/src/gcc/config/loongarch/lasx.md
@@ -186,6 +186,9 @@
   UNSPEC_LASX_XVLDI
   UNSPEC_LASX_XVLDX
   UNSPEC_LASX_XVSTX
+  UNSPEC_LASX_VECINIT_MERGE
+  UNSPEC_LASX_VEC_SET_INTERNAL
+  UNSPEC_LASX_XVILVL_INTERNAL
 ])
 
 ;; All vector modes with 256 bits.
@@ -255,6 +258,15 @@
    [(V8SF "V4SF")
    (V4DF "V2DF")])
 
+;; The attribute gives half int/float modes for vector modes.
+(define_mode_attr VHMODE256_ALL
+  [(V32QI "V16QI")
+   (V16HI "V8HI")
+   (V8SI "V4SI")
+   (V4DI "V2DI")
+   (V8SF "V4SF")
+   (V4DF "V2DF")])
+
 ;; The attribute gives double modes for vector modes in LASX.
 (define_mode_attr VDMODE256
   [(V8SI "V4DI")
@@ -312,6 +324,11 @@
    (V4DI "v4df")
    (V8SI "v8sf")])
 
+;; This attribute gives V32QI mode and V16HI mode with half size.
+(define_mode_attr mode256_i_half
+  [(V32QI "v16qi")
+   (V16HI "v8hi")])
+
  ;; This attribute gives suffix for LASX instructions.  HOW?
 (define_mode_attr lasxfmt
   [(V4DF "d")
@@ -756,6 +773,20 @@
   [(set_attr "type" "simd_splat")
    (set_attr "mode" "<MODE>")])
 
+;; Only for loongarch_expand_vector_init in loongarch.cc.
+;; Support a LSX-mode input op2.
+(define_insn "lasx_vecinit_merge_<LASX:mode>"
+  [(set (match_operand:LASX 0 "register_operand" "=f")
+	(unspec:LASX
+	  [(match_operand:LASX 1 "register_operand" "0")
+	   (match_operand:<VHMODE256_ALL> 2 "register_operand" "f")
+	   (match_operand     3 "const_uimm8_operand")]
+	   UNSPEC_LASX_VECINIT_MERGE))]
+  "ISA_HAS_LASX"
+  "xvpermi.q\t%u0,%u2,%3"
+  [(set_attr "type" "simd_splat")
+   (set_attr "mode" "<MODE>")])
+
 (define_insn "lasx_xvpickve2gr_d<u>"
   [(set (match_operand:DI 0 "register_operand" "=r")
 	(any_extend:DI
@@ -779,6 +810,33 @@
   DONE;
 })
 
+;; Only for loongarch_expand_vector_init in loongarch.cc.
+;; Simulate missing instructions xvinsgr2vr.b and xvinsgr2vr.h.
+(define_expand "vec_set<mode>_internal"
+  [(match_operand:ILASX_HB 0 "register_operand")
+   (match_operand:<UNITMODE> 1 "reg_or_0_operand")
+   (match_operand 2 "const_<indeximm256>_operand")]
+  "ISA_HAS_LASX"
+{
+  rtx index = GEN_INT (1 << INTVAL (operands[2]));
+  emit_insn (gen_lasx_xvinsgr2vr_<mode256_i_half>_internal
+	     (operands[0], operands[1], operands[0], index));
+  DONE;
+})
+
+(define_insn "lasx_xvinsgr2vr_<mode256_i_half>_internal"
+  [(set (match_operand:ILASX_HB 0 "register_operand" "=f")
+	(unspec:ILASX_HB [(match_operand:<UNITMODE> 1 "reg_or_0_operand" "rJ")
+			  (match_operand:ILASX_HB 2 "register_operand" "0")
+			  (match_operand 3 "const_<bitmask256>_operand" "")]
+			 UNSPEC_LASX_VEC_SET_INTERNAL))]
+  "ISA_HAS_LASX"
+{
+  return "vinsgr2vr.<lasxfmt>\t%w0,%z1,%y3";
+}
+  [(set_attr "type" "simd_insert")
+   (set_attr "mode" "<MODE>")])
+
 (define_expand "vec_set<mode>"
   [(match_operand:FLASX 0 "register_operand")
    (match_operand:<UNITMODE> 1 "reg_or_0_operand")
@@ -1567,6 +1625,17 @@
   [(set_attr "type" "simd_flog2")
    (set_attr "mode" "<MODE>")])
 
+;; Only for loongarch_expand_vector_init in loongarch.cc.
+;; Merge two scalar floating-point op1 and op2 into a LASX op0.
+(define_insn "lasx_xvilvl_<lasxfmt_f>_internal"
+  [(set (match_operand:FLASX 0 "register_operand" "=f")
+	(unspec:FLASX [(match_operand:<UNITMODE> 1 "register_operand" "f")
+		       (match_operand:<UNITMODE> 2 "register_operand" "f")]
+		      UNSPEC_LASX_XVILVL_INTERNAL))]
+  "ISA_HAS_LASX"
+  "xvilvl.<lasxfmt>\t%u0,%u2,%u1"
+  [(set_attr "type" "simd_permute")
+   (set_attr "mode" "<MODE>")])
 
 (define_insn "smax<mode>3"
   [(set (match_operand:FLASX 0 "register_operand" "=f")
diff --git a/src/gcc/config/loongarch/loongarch-opts.cc b/src/gcc/config/loongarch/loongarch-opts.cc
index 5cd64184f..f10a9d3ff 100644
--- a/src/gcc/config/loongarch/loongarch-opts.cc
+++ b/src/gcc/config/loongarch/loongarch-opts.cc
@@ -752,8 +752,8 @@ multilib_enabled_abi_list ()
 /* option status feedback for "gcc --help=target -Q" */
 void
 loongarch_update_gcc_opt_status (struct loongarch_target *target,
-        struct gcc_options *opts,
-        struct gcc_options *opts_set)
+				 struct gcc_options *opts,
+				 struct gcc_options *opts_set)
 {
   (void) opts_set;
 
diff --git a/src/gcc/config/loongarch/loongarch-opts.h b/src/gcc/config/loongarch/loongarch-opts.h
index 01566dc8f..f2b59abe6 100644
--- a/src/gcc/config/loongarch/loongarch-opts.h
+++ b/src/gcc/config/loongarch/loongarch-opts.h
@@ -21,6 +21,7 @@ along with GCC; see the file COPYING3.  If not see
 #ifndef LOONGARCH_OPTS_H
 #define LOONGARCH_OPTS_H
 
+#include "loongarch-def.h"
 
 /* Target configuration */
 extern struct loongarch_target la_target;
@@ -32,26 +33,21 @@ struct loongarch_flags {
     int sx[2];
 };
 
-/* Switch masks */
-extern const int loongarch_switch_mask[];
-
-#include "loongarch-def.h"
-
 #if !defined(IN_LIBGCC2) && !defined(IN_TARGET_LIBS) && !defined(IN_RTS)
 
 /* Initialize loongarch_target from separate option variables.  */
 void
 loongarch_init_target (struct loongarch_target *target,
-          int cpu_arch, int cpu_tune, int fpu, int simd,
-          int abi_base, int abi_ext, int cmodel);
+		       int cpu_arch, int cpu_tune, int fpu, int simd,
+		       int abi_base, int abi_ext, int cmodel);
+
 
 /* Handler for "-m" option combinations,
    shared by the driver and the compiler proper.  */
 void
 loongarch_config_target (struct loongarch_target *target,
-      struct loongarch_flags *flags,
-      int follow_multilib_list_p);
-
+			 struct loongarch_flags *flags,
+			 int follow_multilib_list_p);
 
 /* option status feedback for "gcc --help=target -Q" */
 void
@@ -86,27 +82,25 @@ loongarch_update_gcc_opt_status (struct loongarch_target *target,
 				   || la_target.abi.base == ABI_BASE_LP64F \
 				   || la_target.abi.base == ABI_BASE_LP64S)
 
-#define ISA_HAS_LSX      (la_target.isa.simd == ISA_EXT_SIMD_LSX \
-          || la_target.isa.simd == ISA_EXT_SIMD_LASX)
-#define ISA_HAS_LASX     (la_target.isa.simd == ISA_EXT_SIMD_LASX)
+#define ISA_HAS_LSX		  (la_target.isa.simd == ISA_EXT_SIMD_LSX \
+				   || la_target.isa.simd == ISA_EXT_SIMD_LASX)
+#define ISA_HAS_LASX		  (la_target.isa.simd == ISA_EXT_SIMD_LASX)
+
 
 /* TARGET_ macros for use in *.md template conditionals */
-#define TARGET_uARCH_LA464   (la_target.cpu_tune == CPU_LA464)
+#define TARGET_uARCH_LA464	  (la_target.cpu_tune == CPU_LA464)
+#define TARGET_uARCH_LA664	  (la_target.cpu_tune == CPU_LA664)
 
 /* Note: optimize_size may vary across functions,
    while -m[no]-memcpy imposes a global constraint.  */
 #define TARGET_DO_OPTIMIZE_BLOCK_MOVE_P  loongarch_do_optimize_block_move_p()
 
-#ifndef HAVE_AS_MRELAX_OPTION
-#define HAVE_AS_MRELAX_OPTION 0
-#endif
-
-#ifndef HAVE_AS_COND_BRANCH_RELAXATION
-#define HAVE_AS_COND_BRANCH_RELAXATION 0
+#ifndef HAVE_AS_EXPLICIT_RELOCS
+#define HAVE_AS_EXPLICIT_RELOCS 0
 #endif
 
-#ifndef HAVE_AS_TLS
-#define HAVE_AS_TLS 0
+#ifndef HAVE_AS_MRELAX_OPTION
+#define HAVE_AS_MRELAX_OPTION 0
 #endif
 
 #endif /* LOONGARCH_OPTS_H */
diff --git a/src/gcc/config/loongarch/loongarch.cc b/src/gcc/config/loongarch/loongarch.cc
index 4202f7a3f..8b04a3cc1 100644
--- a/src/gcc/config/loongarch/loongarch.cc
+++ b/src/gcc/config/loongarch/loongarch.cc
@@ -7186,6 +7186,36 @@ loongarch_init_machine_status (void)
   return ggc_cleared_alloc<machine_function> ();
 }
 
+static void
+loongarch_cpu_option_override (struct loongarch_target *target,
+			       struct gcc_options *opts,
+			       struct gcc_options *opts_set)
+{
+  /* alignments */
+  if (opts->x_flag_align_functions && !opts->x_str_align_functions)
+    opts->x_str_align_functions
+      = loongarch_cpu_align[target->cpu_tune].function;
+
+  if (opts->x_flag_align_labels && !opts->x_str_align_labels)
+    opts->x_str_align_labels = loongarch_cpu_align[target->cpu_tune].label;
+
+  /* Set up parameters to be used in prefetching algorithm.  */
+  int simultaneous_prefetches
+    = loongarch_cpu_cache[target->cpu_tune].simultaneous_prefetches;
+
+  SET_OPTION_IF_UNSET (opts, opts_set, param_simultaneous_prefetches,
+		       simultaneous_prefetches);
+
+  SET_OPTION_IF_UNSET (opts, opts_set, param_l1_cache_line_size,
+		       loongarch_cpu_cache[target->cpu_tune].l1d_line_size);
+
+  SET_OPTION_IF_UNSET (opts, opts_set, param_l1_cache_size,
+		       loongarch_cpu_cache[target->cpu_tune].l1d_size);
+
+  SET_OPTION_IF_UNSET (opts, opts_set, param_l2_cache_size,
+		       loongarch_cpu_cache[target->cpu_tune].l2d_size);
+}
+
 static void
 loongarch_option_override_internal (struct gcc_options *opts,
 				    struct gcc_options *opts_set)
@@ -7195,12 +7225,16 @@ loongarch_option_override_internal (struct gcc_options *opts,
   if (flag_pic)
     g_switch_value = 0;
 
+  loongarch_init_target (&la_target,
+			 la_opt_cpu_arch, la_opt_cpu_tune, la_opt_fpu,
+			 la_opt_simd, la_opt_abi_base, la_opt_abi_ext,
+			 la_opt_cmodel);
+
   /* Handle target-specific options: compute defaults/conflicts etc.  */
-  loongarch_config_target (&la_target, la_opt_switches,
-			   la_opt_cpu_arch, la_opt_cpu_tune, la_opt_fpu,
-			   la_opt_abi_base, la_opt_abi_ext, la_opt_cmodel, 0);
+  loongarch_config_target (&la_target, NULL, 0);
 
   loongarch_update_gcc_opt_status (&la_target, opts, opts_set);
+  loongarch_cpu_option_override (&la_target, opts, opts_set);
 
   if (TARGET_ABI_LP64)
     flag_pcc_struct_return = 0;
@@ -7298,29 +7332,6 @@ loongarch_option_override (void)
   loongarch_option_override_internal (&global_options, &global_options_set);
 }
 
-/* Implement TARGET_OPTION_SAVE.  */
-static void
-loongarch_option_save (struct cl_target_option *,
-		       struct gcc_options *opts,
-		       struct gcc_options *opts_set)
-{
-  loongarch_update_gcc_opt_status (&la_target, opts, opts_set);
-}
-
-/* Implement TARGET_OPTION_RESTORE.  */
-static void
-loongarch_option_restore (struct gcc_options *,
-			  struct gcc_options *,
-			  struct cl_target_option *ptr)
-{
-  la_target.cpu_arch = ptr->x_la_opt_cpu_arch;
-  la_target.cpu_tune = ptr->x_la_opt_cpu_tune;
-
-  la_target.isa.fpu = ptr->x_la_opt_fpu;
-
-  la_target.cmodel = ptr->x_la_opt_cmodel;
-}
-
 /* Implement TARGET_CONDITIONAL_REGISTER_USAGE.  */
 
 static void
@@ -7798,7 +7809,7 @@ loongarch_expand_lsx_shuffle (struct expand_vec_perm_d *d)
   unsigned i;
 
   if (!ISA_HAS_LSX && !ISA_HAS_LASX)
-    return false;  
+    return false;
 
   for (i = 0; i < d->nelt; i++)
     elts[i] = GEN_INT (d->perm[i]);
@@ -7807,7 +7818,7 @@ loongarch_expand_lsx_shuffle (struct expand_vec_perm_d *d)
   x = gen_rtx_PARALLEL (VOIDmode, v);
 
   if (!loongarch_const_vector_shuffle_set_p (x, d->vmode))
-    return false
+    return false;
 
   x = gen_rtx_VEC_SELECT (d->vmode, d->op0, x);
   x = gen_rtx_SET (d->target, x);
@@ -7845,7 +7856,7 @@ loongarch_expand_vec_perm_interleave (struct expand_vec_perm_d *d)
     return false;
   for (i = 0; i < nelt; i += 2)
     if (d->perm[i] != d->perm[0] + i / 2
- || d->perm[i + 1] != d->perm[0] + i / 2 + nelt)
+	|| d->perm[i + 1] != d->perm[0] + i / 2 + nelt)
       return false;
 
   if (d->testing_p)
@@ -7889,28 +7900,28 @@ loongarch_expand_vec_perm_interleave (struct expand_vec_perm_d *d)
     {
       t3 = gen_reg_rtx (V4DFmode);
       if (d->perm[0])
- emit_insn (gen_lasx_xvpermi_q_v4df (t3, gen_lowpart (V4DFmode, t1),
-             gen_lowpart (V4DFmode, t2),
-             GEN_INT (0x31)));
+	emit_insn (gen_lasx_xvpermi_q_v4df (t3, gen_lowpart (V4DFmode, t1),
+					    gen_lowpart (V4DFmode, t2),
+					    GEN_INT (0x31)));
       else
- emit_insn (gen_lasx_xvpermi_q_v4df (t3, gen_lowpart (V4DFmode, t1),
-             gen_lowpart (V4DFmode, t2),
-             GEN_INT (0x20)));
+	emit_insn (gen_lasx_xvpermi_q_v4df (t3, gen_lowpart (V4DFmode, t1),
+					    gen_lowpart (V4DFmode, t2),
+					    GEN_INT (0x20)));
     }
   else
     {
       t3 = gen_reg_rtx (V4DImode);
       if (d->perm[0])
- emit_insn (gen_lasx_xvpermi_q_v4di (t3, gen_lowpart (V4DImode, t1),
-             gen_lowpart (V4DImode, t2),
-             GEN_INT (0x31)));
+	emit_insn (gen_lasx_xvpermi_q_v4di (t3, gen_lowpart (V4DImode, t1),
+					    gen_lowpart (V4DImode, t2),
+					    GEN_INT (0x31)));
       else
- emit_insn (gen_lasx_xvpermi_q_v4di (t3, gen_lowpart (V4DImode, t1),
-             gen_lowpart (V4DImode, t2),
-             GEN_INT (0x20)));
+	emit_insn (gen_lasx_xvpermi_q_v4di (t3, gen_lowpart (V4DImode, t1),
+					    gen_lowpart (V4DImode, t2),
+					    GEN_INT (0x20)));
     }
- emit_move_insn (d->target, gen_lowpart (mode, t3));
- return true;
+  emit_move_insn (d->target, gen_lowpart (mode, t3));
+  return true;
 }
 
 /* Implement extract-even and extract-odd permutations.  */
@@ -7931,60 +7942,60 @@ loongarch_expand_vec_perm_even_odd_1 (struct expand_vec_perm_d *d, unsigned odd)
     case E_V4DFmode:
       /* Shuffle the lanes around into { 0 4 2 6 } and { 1 5 3 7 }.  */
       if (odd)
- emit_insn (gen_lasx_xvilvh_d_f (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvilvh_d_f (t1, d->op0, d->op1));
       else
- emit_insn (gen_lasx_xvilvl_d_f (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvilvl_d_f (t1, d->op0, d->op1));
 
       /* Shuffle within the 256-bit lanes to produce the result required.
-  { 0 2 4 6 } | { 1 3 5 7 }.  */
+	 { 0 2 4 6 } | { 1 3 5 7 }.  */
       emit_insn (gen_lasx_xvpermi_d_v4df (d->target, t1, GEN_INT (0xd8)));
       break;
 
     case E_V4DImode:
       if (odd)
- emit_insn (gen_lasx_xvilvh_d (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvilvh_d (t1, d->op0, d->op1));
       else
- emit_insn (gen_lasx_xvilvl_d (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvilvl_d (t1, d->op0, d->op1));
 
       emit_insn (gen_lasx_xvpermi_d_v4di (d->target, t1, GEN_INT (0xd8)));
       break;
 
     case E_V8SFmode:
       /* Shuffle the lanes around into:
-  { 0 2 8 a 4 6 c e } | { 1 3 9 b 5 7 d f }.  */
+	 { 0 2 8 a 4 6 c e } | { 1 3 9 b 5 7 d f }.  */
       if (odd)
- emit_insn (gen_lasx_xvpickod_w_f (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvpickod_w_f (t1, d->op0, d->op1));
       else
- emit_insn (gen_lasx_xvpickev_w_f (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvpickev_w_f (t1, d->op0, d->op1));
 
       /* Shuffle within the 256-bit lanes to produce the result required.
-  { 0 2 4 6 8 a c e } | { 1 3 5 7 9 b d f }.  */
+	 { 0 2 4 6 8 a c e } | { 1 3 5 7 9 b d f }.  */
       emit_insn (gen_lasx_xvpermi_d_v8sf (d->target, t1, GEN_INT (0xd8)));
       break;
 
     case E_V8SImode:
       if (odd)
- emit_insn (gen_lasx_xvpickod_w (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvpickod_w (t1, d->op0, d->op1));
       else
- emit_insn (gen_lasx_xvpickev_w (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvpickev_w (t1, d->op0, d->op1));
 
       emit_insn (gen_lasx_xvpermi_d_v8si (d->target, t1, GEN_INT (0xd8)));
       break;
 
     case E_V16HImode:
       if (odd)
- emit_insn (gen_lasx_xvpickod_h (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvpickod_h (t1, d->op0, d->op1));
       else
- emit_insn (gen_lasx_xvpickev_h (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvpickev_h (t1, d->op0, d->op1));
 
       emit_insn (gen_lasx_xvpermi_d_v16hi (d->target, t1, GEN_INT (0xd8)));
       break;
 
     case E_V32QImode:
       if (odd)
- emit_insn (gen_lasx_xvpickod_b (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvpickod_b (t1, d->op0, d->op1));
       else
- emit_insn (gen_lasx_xvpickev_b (t1, d->op0, d->op1));
+	emit_insn (gen_lasx_xvpickev_b (t1, d->op0, d->op1));
 
       emit_insn (gen_lasx_xvpermi_d_v32qi (d->target, t1, GEN_INT (0xd8)));
       break;
@@ -8044,51 +8055,51 @@ loongarch_expand_vec_perm_1 (rtx operands[])
   if (mode != E_V32QImode)
     {
       for (int i = 0; i < w; i += 1)
- {
-   round_data[i] = GEN_INT (0x1f);
- }
+	{
+	  round_data[i] = GEN_INT (0x1f);
+	}
 
       if (mode == E_V4DFmode)
- {
-   round_data_rtx = gen_rtx_CONST_VECTOR (E_V4DImode,
-            gen_rtvec_v (w, round_data));
-   round_reg = gen_reg_rtx (E_V4DImode);
- }
+	{
+	  round_data_rtx = gen_rtx_CONST_VECTOR (E_V4DImode,
+						 gen_rtvec_v (w, round_data));
+	  round_reg = gen_reg_rtx (E_V4DImode);
+	}
       else if (mode == E_V8SFmode)
- {
+	{
 
-   round_data_rtx = gen_rtx_CONST_VECTOR (E_V8SImode,
-            gen_rtvec_v (w, round_data));
-   round_reg = gen_reg_rtx (E_V8SImode);
- }
+	  round_data_rtx = gen_rtx_CONST_VECTOR (E_V8SImode,
+						 gen_rtvec_v (w, round_data));
+	  round_reg = gen_reg_rtx (E_V8SImode);
+	}
       else
- {
-   round_data_rtx = gen_rtx_CONST_VECTOR (mode,
-            gen_rtvec_v (w, round_data));
-   round_reg = gen_reg_rtx (mode);
- }
+	{
+	  round_data_rtx = gen_rtx_CONST_VECTOR (mode,
+						 gen_rtvec_v (w, round_data));
+	  round_reg = gen_reg_rtx (mode);
+	}
 
       emit_move_insn (round_reg, round_data_rtx);
       switch (mode)
- {
- case E_V32QImode:
-   emit_insn (gen_andv32qi3 (mask, mask, round_reg));
-   break;
- case E_V16HImode:
-   emit_insn (gen_andv16hi3 (mask, mask, round_reg));
-   break;
- case E_V8SImode:
- case E_V8SFmode:
-   emit_insn (gen_andv8si3 (mask, mask, round_reg));
-   break;
- case E_V4DImode:
- case E_V4DFmode:
-   emit_insn (gen_andv4di3 (mask, mask, round_reg));
-   break;
- default:
-   gcc_unreachable ();
-   break;
- }
+	{
+	case E_V32QImode:
+	  emit_insn (gen_andv32qi3 (mask, mask, round_reg));
+	  break;
+	case E_V16HImode:
+	  emit_insn (gen_andv16hi3 (mask, mask, round_reg));
+	  break;
+	case E_V8SImode:
+	case E_V8SFmode:
+	  emit_insn (gen_andv8si3 (mask, mask, round_reg));
+	  break;
+	case E_V4DImode:
+	case E_V4DFmode:
+	  emit_insn (gen_andv4di3 (mask, mask, round_reg));
+	  break;
+	default:
+	  gcc_unreachable ();
+	  break;
+	}
     }
 
   if (mode == V4DImode || mode == V4DFmode)
@@ -8098,10 +8109,10 @@ loongarch_expand_vec_perm_1 (rtx operands[])
       t1 = gen_reg_rtx (maskmode);
 
       /* Replicate the low bits of the V4DImode mask into V8SImode:
-  mask = { A B C D }
-  t1 = { A A B B C C D D }.  */
+	 mask = { A B C D }
+	 t1 = { A A B B C C D D }.  */
       for (i = 0; i < w / 2; ++i)
- vec[i*2 + 1] = vec[i*2] = GEN_INT (i * 2);
+	vec[i*2 + 1] = vec[i*2] = GEN_INT (i * 2);
       vt = gen_rtx_CONST_VECTOR (maskmode, gen_rtvec_v (w, vec));
       vt = force_reg (maskmode, vt);
       mask = gen_lowpart (maskmode, mask);
@@ -8109,19 +8120,19 @@ loongarch_expand_vec_perm_1 (rtx operands[])
 
       /* Multiply the shuffle indicies by two.  */
       t1 = expand_simple_binop (maskmode, PLUS, t1, t1, t1, 1,
-       OPTAB_DIRECT);
+				OPTAB_DIRECT);
 
       /* Add one to the odd shuffle indicies:
-  t1 = { A*2, A*2+1, B*2, B*2+1, ... }.  */
+	 t1 = { A*2, A*2+1, B*2, B*2+1, ... }.  */
       for (i = 0; i < w / 2; ++i)
- {
-   vec[i * 2] = const0_rtx;
-   vec[i * 2 + 1] = const1_rtx;
- }
+	{
+	  vec[i * 2] = const0_rtx;
+	  vec[i * 2 + 1] = const1_rtx;
+	}
       vt = gen_rtx_CONST_VECTOR (maskmode, gen_rtvec_v (w, vec));
       vt = validize_mem (force_const_mem (maskmode, vt));
       t1 = expand_simple_binop (maskmode, PLUS, t1, vt, t1, 1,
-       OPTAB_DIRECT);
+				OPTAB_DIRECT);
 
       /* Continue as if V8SImode (resp.  V32QImode) was used initially.  */
       operands[3] = mask = t1;
@@ -8129,92 +8140,93 @@ loongarch_expand_vec_perm_1 (rtx operands[])
       op0 = gen_lowpart (mode, op0);
       op1 = gen_lowpart (mode, op1);
     }
+
   switch (mode)
     {
     case E_V8SImode:
       if (one_operand_shuffle)
- {
-   emit_insn (gen_lasx_xvperm_w (target, op0, mask));
-   if (target != operands[0])
-     emit_move_insn (operands[0],
-         gen_lowpart (GET_MODE (operands[0]), target));
- }
+	{
+	  emit_insn (gen_lasx_xvperm_w (target, op0, mask));
+	  if (target != operands[0])
+	    emit_move_insn (operands[0],
+			    gen_lowpart (GET_MODE (operands[0]), target));
+	}
       else
- {
-   t1 = gen_reg_rtx (V8SImode);
-   t2 = gen_reg_rtx (V8SImode);
-   emit_insn (gen_lasx_xvperm_w (t1, op0, mask));
-   emit_insn (gen_lasx_xvperm_w (t2, op1, mask));
-   goto merge_two;
- }
+	{
+	  t1 = gen_reg_rtx (V8SImode);
+	  t2 = gen_reg_rtx (V8SImode);
+	  emit_insn (gen_lasx_xvperm_w (t1, op0, mask));
+	  emit_insn (gen_lasx_xvperm_w (t2, op1, mask));
+	  goto merge_two;
+	}
       return;
 
     case E_V8SFmode:
       mask = gen_lowpart (V8SImode, mask);
       if (one_operand_shuffle)
- emit_insn (gen_lasx_xvperm_w_f (target, op0, mask));
+	emit_insn (gen_lasx_xvperm_w_f (target, op0, mask));
       else
- {
-   t1 = gen_reg_rtx (V8SFmode);
-   t2 = gen_reg_rtx (V8SFmode);
-   emit_insn (gen_lasx_xvperm_w_f (t1, op0, mask));
-   emit_insn (gen_lasx_xvperm_w_f (t2, op1, mask));
-   goto merge_two;
- }
+	{
+	  t1 = gen_reg_rtx (V8SFmode);
+	  t2 = gen_reg_rtx (V8SFmode);
+	  emit_insn (gen_lasx_xvperm_w_f (t1, op0, mask));
+	  emit_insn (gen_lasx_xvperm_w_f (t2, op1, mask));
+	  goto merge_two;
+	}
       return;
 
     case E_V16HImode:
       if (one_operand_shuffle)
- {
-   t1 = gen_reg_rtx (V16HImode);
-   t2 = gen_reg_rtx (V16HImode);
-   emit_insn (gen_lasx_xvpermi_d_v16hi (t1, op0, GEN_INT (0x44)));
-   emit_insn (gen_lasx_xvpermi_d_v16hi (t2, op0, GEN_INT (0xee)));
-   emit_insn (gen_lasx_xvshuf_h (target, mask, t2, t1));
- }
+	{
+	  t1 = gen_reg_rtx (V16HImode);
+	  t2 = gen_reg_rtx (V16HImode);
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t1, op0, GEN_INT (0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t2, op0, GEN_INT (0xee)));
+	  emit_insn (gen_lasx_xvshuf_h (target, mask, t2, t1));
+	}
       else
-       {
-         t1 = gen_reg_rtx (V16HImode);
-         t2 = gen_reg_rtx (V16HImode);
-         t3 = gen_reg_rtx (V16HImode);
-         t4 = gen_reg_rtx (V16HImode);
-         t5 = gen_reg_rtx (V16HImode);
-         t6 = gen_reg_rtx (V16HImode);
-         emit_insn (gen_lasx_xvpermi_d_v16hi (t3, op0, GEN_INT (0x44)));
-         emit_insn (gen_lasx_xvpermi_d_v16hi (t4, op0, GEN_INT (0xee)));
-         emit_insn (gen_lasx_xvshuf_h (t1, mask, t4, t3));
-         emit_insn (gen_lasx_xvpermi_d_v16hi (t5, op1, GEN_INT (0x44)));
-         emit_insn (gen_lasx_xvpermi_d_v16hi (t6, op1, GEN_INT (0xee)));
-         emit_insn (gen_lasx_xvshuf_h (t2, mask, t6, t5));
-         goto merge_two;
-       }
+	{
+	  t1 = gen_reg_rtx (V16HImode);
+	  t2 = gen_reg_rtx (V16HImode);
+	  t3 = gen_reg_rtx (V16HImode);
+	  t4 = gen_reg_rtx (V16HImode);
+	  t5 = gen_reg_rtx (V16HImode);
+	  t6 = gen_reg_rtx (V16HImode);
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t3, op0, GEN_INT (0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t4, op0, GEN_INT (0xee)));
+	  emit_insn (gen_lasx_xvshuf_h (t1, mask, t4, t3));
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t5, op1, GEN_INT (0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t6, op1, GEN_INT (0xee)));
+	  emit_insn (gen_lasx_xvshuf_h (t2, mask, t6, t5));
+	  goto merge_two;
+	}
       return;
 
     case E_V32QImode:
       if (one_operand_shuffle)
-       {
-         t1 = gen_reg_rtx (V32QImode);
-         t2 = gen_reg_rtx (V32QImode);
-         emit_insn (gen_lasx_xvpermi_d_v32qi (t1, op0, GEN_INT (0x44)));
-         emit_insn (gen_lasx_xvpermi_d_v32qi (t2, op0, GEN_INT (0xee)));
-         emit_insn (gen_lasx_xvshuf_b (target, t2, t1, mask));
-       }
+	{
+	  t1 = gen_reg_rtx (V32QImode);
+	  t2 = gen_reg_rtx (V32QImode);
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t1, op0, GEN_INT (0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t2, op0, GEN_INT (0xee)));
+	  emit_insn (gen_lasx_xvshuf_b (target, t2, t1, mask));
+	}
       else
-       {
-         t1 = gen_reg_rtx (V32QImode);
-         t2 = gen_reg_rtx (V32QImode);
-         t3 = gen_reg_rtx (V32QImode);
-         t4 = gen_reg_rtx (V32QImode);
-         t5 = gen_reg_rtx (V32QImode);
-         t6 = gen_reg_rtx (V32QImode);
-         emit_insn (gen_lasx_xvpermi_d_v32qi (t3, op0, GEN_INT (0x44)));
-         emit_insn (gen_lasx_xvpermi_d_v32qi (t4, op0, GEN_INT (0xee)));
-         emit_insn (gen_lasx_xvshuf_b (t1, t4, t3, mask));
-         emit_insn (gen_lasx_xvpermi_d_v32qi (t5, op1, GEN_INT (0x44)));
-         emit_insn (gen_lasx_xvpermi_d_v32qi (t6, op1, GEN_INT (0xee)));
-         emit_insn (gen_lasx_xvshuf_b (t2, t6, t5, mask));
-         goto merge_two;
-       }
+	{
+	  t1 = gen_reg_rtx (V32QImode);
+	  t2 = gen_reg_rtx (V32QImode);
+	  t3 = gen_reg_rtx (V32QImode);
+	  t4 = gen_reg_rtx (V32QImode);
+	  t5 = gen_reg_rtx (V32QImode);
+	  t6 = gen_reg_rtx (V32QImode);
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t3, op0, GEN_INT (0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t4, op0, GEN_INT (0xee)));
+	  emit_insn (gen_lasx_xvshuf_b (t1, t4, t3, mask));
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t5, op1, GEN_INT (0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t6, op1, GEN_INT (0xee)));
+	  emit_insn (gen_lasx_xvshuf_b (t2, t6, t5, mask));
+	  goto merge_two;
+	}
       return;
 
     default:
@@ -8231,7 +8243,7 @@ merge_two:
   vt = gen_const_vec_duplicate (maskmode, vt);
   vt = force_reg (maskmode, vt);
   mask = expand_simple_binop (maskmode, AND, mask, vt,
-                             NULL_RTX, 0, OPTAB_DIRECT);
+			      NULL_RTX, 0, OPTAB_DIRECT);
   if (GET_MODE (target) != mode)
     target = gen_reg_rtx (mode);
   xops[0] = target;
@@ -8244,7 +8256,7 @@ merge_two:
   loongarch_expand_vec_cond_expr (mode, maskmode, xops);
   if (target != operands[0])
     emit_move_insn (operands[0],
-                   gen_lowpart (GET_MODE (operands[0]), target));
+		    gen_lowpart (GET_MODE (operands[0]), target));
 }
 
 void
@@ -8302,66 +8314,66 @@ loongarch_try_expand_lsx_vshuf_const (struct expand_vec_perm_d *d)
   rtx rperm[MAX_VECT_LEN];
 
   if (d->vmode == E_V2DImode || d->vmode == E_V2DFmode
-       || d->vmode == E_V4SImode || d->vmode == E_V4SFmode
-       || d->vmode == E_V8HImode || d->vmode == E_V16QImode)
+	|| d->vmode == E_V4SImode || d->vmode == E_V4SFmode
+	|| d->vmode == E_V8HImode || d->vmode == E_V16QImode)
     {
       target = d->target;
       op0 = d->op0;
       op1 = d->one_vector_p ? d->op0 : d->op1;
 
       if (GET_MODE (op0) != GET_MODE (op1)
-   || GET_MODE (op0) != GET_MODE (target))
- return false;
+	  || GET_MODE (op0) != GET_MODE (target))
+	return false;
 
       if (d->testing_p)
- return true
+	return true;
 
       for (i = 0; i < d->nelt; i += 1)
- {
-   rperm[i] = GEN_INT (d->perm[i]);
- }
+	{
+	  rperm[i] = GEN_INT (d->perm[i]);
+	}
 
       if (d->vmode == E_V2DFmode)
- {
-   sel = gen_rtx_CONST_VECTOR (E_V2DImode, gen_rtvec_v (d->nelt, rperm));
-   tmp = gen_rtx_SUBREG (E_V2DImode, d->target, 0);
-   emit_move_insn (tmp, sel);
- }
+	{
+	  sel = gen_rtx_CONST_VECTOR (E_V2DImode, gen_rtvec_v (d->nelt, rperm));
+	  tmp = gen_rtx_SUBREG (E_V2DImode, d->target, 0);
+	  emit_move_insn (tmp, sel);
+	}
       else if (d->vmode == E_V4SFmode)
- {
-   sel = gen_rtx_CONST_VECTOR (E_V4SImode, gen_rtvec_v (d->nelt, rperm));
-   tmp = gen_rtx_SUBREG (E_V4SImode, d->target, 0);
-   emit_move_insn (tmp, sel);
- }
+	{
+	  sel = gen_rtx_CONST_VECTOR (E_V4SImode, gen_rtvec_v (d->nelt, rperm));
+	  tmp = gen_rtx_SUBREG (E_V4SImode, d->target, 0);
+	  emit_move_insn (tmp, sel);
+	}
       else
- {
-   sel = gen_rtx_CONST_VECTOR (d->vmode, gen_rtvec_v (d->nelt, rperm));
-   emit_move_insn (d->target, sel);
- }
+	{
+	  sel = gen_rtx_CONST_VECTOR (d->vmode, gen_rtvec_v (d->nelt, rperm));
+	  emit_move_insn (d->target, sel);
+	}
 
       switch (d->vmode)
- {
- case E_V2DFmode:
-   emit_insn (gen_lsx_vshuf_d_f (target, target, op1, op0));
-   break;
- case E_V2DImode:
-   emit_insn (gen_lsx_vshuf_d (target, target, op1, op0));
-   break;
- case E_V4SFmode:
-   emit_insn (gen_lsx_vshuf_w_f (target, target, op1, op0));
-   break;
- case E_V4SImode:
-   emit_insn (gen_lsx_vshuf_w (target, target, op1, op0));
-   break;
- case E_V8HImode:
-   emit_insn (gen_lsx_vshuf_h (target, target, op1, op0));
-   break;
- case E_V16QImode:
-   emit_insn (gen_lsx_vshuf_b (target, op1, op0, target));
-   break;
- default:
-   break;
- }
+	{
+	case E_V2DFmode:
+	  emit_insn (gen_lsx_vshuf_d_f (target, target, op1, op0));
+	  break;
+	case E_V2DImode:
+	  emit_insn (gen_lsx_vshuf_d (target, target, op1, op0));
+	  break;
+	case E_V4SFmode:
+	  emit_insn (gen_lsx_vshuf_w_f (target, target, op1, op0));
+	  break;
+	case E_V4SImode:
+	  emit_insn (gen_lsx_vshuf_w (target, target, op1, op0));
+	  break;
+	case E_V8HImode:
+	  emit_insn (gen_lsx_vshuf_h (target, target, op1, op0));
+	  break;
+	case E_V16QImode:
+	  emit_insn (gen_lsx_vshuf_b (target, op1, op0, target));
+	  break;
+	default:
+	  break;
+	}
 
       return true;
     }
@@ -8379,23 +8391,23 @@ loongarch_expand_vec_perm_const_1 (struct expand_vec_perm_d *d)
       /* Try interleave with alternating operands.  */
       memcpy (perm2, d->perm, sizeof (perm2));
       for (i = 1; i < nelt; i += 2)
- perm2[i] += nelt;
+	perm2[i] += nelt;
       if (loongarch_expand_vselect_vconcat (d->target, d->op0, d->op1, perm2,
-             nelt))
- return true;
+					    nelt))
+	return true;
     }
   else
     {
       if (loongarch_expand_vselect_vconcat (d->target, d->op0, d->op1,
-             d->perm, nelt))
- return true;
+					    d->perm, nelt))
+	return true;
 
       /* Try again with swapped operands.  */
       for (i = 0; i < nelt; ++i)
- perm2[i] = (d->perm[i] + nelt) & (2 * nelt - 1);
+	perm2[i] = (d->perm[i] + nelt) & (2 * nelt - 1);
       if (loongarch_expand_vselect_vconcat (d->target, d->op1, d->op0, perm2,
-             nelt))
- return true;
+					    nelt))
+	return true;
     }
 
   if (loongarch_expand_lsx_shuffle (d))
@@ -8424,15 +8436,15 @@ loongarch_is_quad_duplicate (struct expand_vec_perm_d *d)
   for (int i = 1; i < d->nelt; i += 1)
     {
       if ((i < d->nelt / 2) && (d->perm[i] != lhs))
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
       if ((i > d->nelt / 2) && (d->perm[i] != rhs))
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
     }
 
   return result;
@@ -8453,15 +8465,15 @@ loongarch_is_double_duplicate (struct expand_vec_perm_d *d)
   for (int i = 1; i < d->nelt; i += 2)
     {
       if (d->perm[i] != buf)
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
       if (d->perm[i - 1] != d->perm[i])
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
       buf += d->nelt / 4;
     }
 
@@ -8477,10 +8489,10 @@ loongarch_is_odd_extraction (struct expand_vec_perm_d *d)
   for (int i = 0; i < d->nelt; i += 1)
     {
       if (buf != d->perm[i])
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
       buf += 2;
     }
 
@@ -8496,10 +8508,10 @@ loongarch_is_even_extraction (struct expand_vec_perm_d *d)
   for (int i = 0; i < d->nelt; i += 1)
     {
       if (buf != d->perm[i])
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
       buf += 1;
     }
 
@@ -8518,10 +8530,10 @@ loongarch_is_extraction_permutation (struct expand_vec_perm_d *d)
   for (int i = 0; i < d->nelt; i += 1)
     {
       if (buf != d->perm[i])
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
       buf += 2;
     }
 
@@ -8537,10 +8549,10 @@ loongarch_is_center_extraction (struct expand_vec_perm_d *d)
   for (int i = 0; i < d->nelt; i += 1)
     {
       if (buf != d->perm[i])
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
       buf += 1;
     }
 
@@ -8559,10 +8571,10 @@ loongarch_is_reversing_permutation (struct expand_vec_perm_d *d)
   for (int i = 0; i < d->nelt; i += 1)
     {
       if (d->perm[i] != buf)
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
 
       buf -= 1;
     }
@@ -8583,29 +8595,29 @@ loongarch_is_di_misalign_extract (struct expand_vec_perm_d *d)
     {
       buf = 1;
       for (int i = 0; i < d->nelt; i += 1)
-       {
-         if (buf != d->perm[i])
-           {
-             result = false;
-             break;
-           }
+	{
+	  if (buf != d->perm[i])
+	    {
+	      result = false;
+	      break;
+	    }
 
-         buf += 1;
-       }
+	  buf += 1;
+	}
     }
   else if (d->nelt == 8)
     {
       buf = 2;
       for (int i = 0; i < d->nelt; i += 1)
-       {
-         if (buf != d->perm[i])
-           {
-             result = false;
-             break;
-           }
+	{
+	  if (buf != d->perm[i])
+	    {
+	      result = false;
+	      break;
+	    }
 
-         buf += 1;
-       }
+	  buf += 1;
+	}
     }
 
   return result;
@@ -8622,10 +8634,10 @@ loongarch_is_si_misalign_extract (struct expand_vec_perm_d *d)
   for (int i = 0; i < d->nelt; i += 1)
     {
       if (buf != d->perm[i])
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
       buf += 1;
     }
 
@@ -8641,10 +8653,10 @@ loongarch_is_lasx_lowpart_interleave (struct expand_vec_perm_d *d)
   for (int i = 0;i < d->nelt; i += 2)
     {
       if (buf != d->perm[i])
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
       buf += 1;
     }
 
@@ -8652,14 +8664,14 @@ loongarch_is_lasx_lowpart_interleave (struct expand_vec_perm_d *d)
     {
       buf = d->nelt;
       for (int i = 1; i < d->nelt; i += 2)
-       {
-         if (buf != d->perm[i])
-           {
-             result = false;
-             break;
-           }
-         buf += 1;
-       }
+	{
+	  if (buf != d->perm[i])
+	    {
+	      result = false;
+	      break;
+	    }
+	  buf += 1;
+	}
     }
 
   return result;
@@ -8678,10 +8690,10 @@ loongarch_is_lasx_lowpart_interleave_2 (struct expand_vec_perm_d *d)
   for (int i = BEGIN; i < END && result; i += 1) \
     { \
       if (buf != d->perm[i]) \
-       { \
-         result = false; \
-         break; \
-       } \
+	{ \
+	  result = false; \
+	  break; \
+	} \
       buf += 1; \
     }
 
@@ -8703,10 +8715,10 @@ loongarch_is_lasx_lowpart_extract (struct expand_vec_perm_d *d)
   for (int i = 0; i < d->nelt / 2; i += 1)
     {
       if (buf != d->perm[i])
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
       buf += 1;
     }
 
@@ -8714,14 +8726,14 @@ loongarch_is_lasx_lowpart_extract (struct expand_vec_perm_d *d)
     {
       buf = d->nelt;
       for (int i = d->nelt / 2; i < d->nelt; i += 1)
-       {
-         if (buf != d->perm[i])
-           {
-             result = false;
-             break;
-           }
-         buf += 1;
-       }
+	{
+	  if (buf != d->perm[i])
+	    {
+	      result = false;
+	      break;
+	    }
+	  buf += 1;
+	}
     }
 
   return result;
@@ -8736,10 +8748,10 @@ loongarch_is_lasx_highpart_interleave (expand_vec_perm_d *d)
   for (int i = 0; i < d->nelt; i += 2)
     {
       if (buf != d->perm[i])
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
       buf += 1;
     }
 
@@ -8747,14 +8759,14 @@ loongarch_is_lasx_highpart_interleave (expand_vec_perm_d *d)
     {
       buf = d->nelt + d->nelt / 2;
       for (int i = 1; i < d->nelt;i += 2)
-       {
-         if (buf != d->perm[i])
-           {
-             result = false;
-             break;
-           }
-         buf += 1;
-       }
+	{
+	  if (buf != d->perm[i])
+	    {
+	      result = false;
+	      break;
+	    }
+	  buf += 1;
+	}
     }
 
   return result;
@@ -8774,10 +8786,10 @@ loongarch_is_lasx_highpart_interleave_2 (struct expand_vec_perm_d *d)
   for (int i = BEGIN; i < END && result; i += 1) \
     { \
       if (buf != d->perm[i]) \
-       { \
-         result = false; \
-         break; \
-       } \
+	{ \
+	  result = false; \
+	  break; \
+	} \
       buf += 1; \
     }
 
@@ -8799,10 +8811,10 @@ loongarch_is_elem_duplicate (struct expand_vec_perm_d *d)
   for (int i = 0; i < d->nelt; i += 1)
     {
       if (buf != d->perm[i])
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
     }
 
   return result;
@@ -8824,10 +8836,10 @@ loongarch_is_single_op_perm (struct expand_vec_perm_d *d)
   for (int i = 0; i < d->nelt; i += 1)
     {
       if (d->perm[i] >= d->nelt)
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
     }
 
   return result;
@@ -8841,22 +8853,22 @@ loongarch_is_divisible_perm (struct expand_vec_perm_d *d)
   for (int i = 0; i < d->nelt / 2; i += 1)
     {
       if (d->perm[i] >= d->nelt)
-       {
-         result = false;
-         break;
-       }
+	{
+	  result = false;
+	  break;
+	}
     }
 
   if (result)
     {
       for (int i = d->nelt / 2; i < d->nelt; i += 1)
-       {
-         if (d->perm[i] < d->nelt)
-           {
-             result = false;
-             break;
-           }
-       }
+	{
+	  if (d->perm[i] < d->nelt)
+	    {
+	      result = false;
+	      break;
+	    }
+	}
     }
 
   return result;
@@ -8886,15 +8898,15 @@ loongarch_is_triple_stride_extract (struct expand_vec_perm_d *d)
  * Two op registers (op0, op1) will "combine" into a 512bit temp vector storage
  * that has 2*nelt elements inside it; the low 256bit is op0, and high 256bit
  * is op1, then the elements are indexed as below:
- *               0 ~ nelt - 1          nelt ~ 2 * nelt - 1
- *       |-------------------------|-------------------------|
- *             Low 256bit (op0)        High 256bit (op1)
+ *		  0 ~ nelt - 1		nelt ~ 2 * nelt - 1
+ *	  |-------------------------|-------------------------|
+ *		Low 256bit (op0)	High 256bit (op1)
  * For example, the second element in op1 (V8SImode) will be indexed with 9.
  * Selector is a vector that has the same mode and number of elements  with
  * op0,op1 and target, it's look like this:
- *           0 ~ nelt - 1
- *       |-------------------------|
- *           256bit (selector)
+ *	      0 ~ nelt - 1
+ *	  |-------------------------|
+ *	      256bit (selector)
  * It describes which element from 512bit temp vector storage will fit into
  * target's every element slot.
  * GCC expects that every element in selector can be ANY indices of 512bit
@@ -8911,14 +8923,14 @@ loongarch_is_triple_stride_extract (struct expand_vec_perm_d *d)
  *
  *  a) op0's low 128bit and op1's low 128bit "combines" into a 256bit temp
  * vector storage (TVS1), elements are indexed as below:
- *         0 ~ nelt / 2 - 1      nelt / 2 ~ nelt - 1
- *     |---------------------|---------------------| TVS1
- *         op0's low 128bit      op1's low 128bit
+ *	    0 ~ nelt / 2 - 1	  nelt / 2 ~ nelt - 1
+ *	|---------------------|---------------------| TVS1
+ *	    op0's low 128bit      op1's low 128bit
  *    op0's high 128bit and op1's high 128bit are "combined" into TVS2 in the
  *    same way.
- *         0 ~ nelt / 2 - 1      nelt / 2 ~ nelt - 1
- *     |---------------------|---------------------| TVS2
- *         op0's high 128bit   op1's high 128bit
+ *	    0 ~ nelt / 2 - 1	  nelt / 2 ~ nelt - 1
+ *	|---------------------|---------------------| TVS2
+ *	    op0's high 128bit	op1's high 128bit
  *  b) Selector's low 128bit describes which elements from TVS1 will fit into
  *  target vector's low 128bit.  No TVS2 elements are allowed.
  *  c) Selector's high 128bit describes which elements from TVS2 will fit into
@@ -8972,39 +8984,39 @@ loongarch_expand_vec_perm_const_2 (struct expand_vec_perm_d *d)
   if (loongarch_is_quad_duplicate (d))
     {
       /* Selector example: E_V8SImode, { 0, 0, 0, 0, 4, 4, 4, 4 }
-        copy first elem from original selector to all elem in new selector.  */
+	 copy first elem from original selector to all elem in new selector.  */
       idx = d->perm[0];
       for (i = 0; i < d->nelt; i += 1)
-       {
-         remapped[i] = idx;
-       }
+	{
+	  remapped[i] = idx;
+	}
       /* Selector after: { 0, 0, 0, 0, 0, 0, 0, 0 }.  */
     }
   else if (loongarch_is_double_duplicate (d))
     {
       /* Selector example: E_V8SImode, { 1, 1, 3, 3, 5, 5, 7, 7 }
-        one_vector_p == true.  */
+	 one_vector_p == true.  */
       for (i = 0; i < d->nelt / 2; i += 1)
-       {
-         idx = d->perm[i];
-         remapped[i] = idx;
-         remapped[i + d->nelt / 2] = idx;
-       }
+	{
+	  idx = d->perm[i];
+	  remapped[i] = idx;
+	  remapped[i + d->nelt / 2] = idx;
+	}
       /* Selector after: { 1, 1, 3, 3, 1, 1, 3, 3 }.  */
     }
   else if (loongarch_is_odd_extraction (d)
-          || loongarch_is_even_extraction (d))
+	   || loongarch_is_even_extraction (d))
     {
       /* Odd extraction selector sample: E_V4DImode, { 1, 3, 5, 7 }
-        Selector after: { 1, 3, 1, 3 }.
-        Even extraction selector sample: E_V4DImode, { 0, 2, 4, 6 }
-        Selector after: { 0, 2, 0, 2 }.  */
+	 Selector after: { 1, 3, 1, 3 }.
+	 Even extraction selector sample: E_V4DImode, { 0, 2, 4, 6 }
+	 Selector after: { 0, 2, 0, 2 }.  */
       for (i = 0; i < d->nelt / 2; i += 1)
-       {
-         idx = d->perm[i];
-         remapped[i] = idx;
-         remapped[i + d->nelt / 2] = idx;
-       }
+	{
+	  idx = d->perm[i];
+	  remapped[i] = idx;
+	  remapped[i + d->nelt / 2] = idx;
+	}
       /* Additional insn is required for correct result.  See codes below.  */
       extract_ev_od = true;
     }
@@ -9012,64 +9024,64 @@ loongarch_expand_vec_perm_const_2 (struct expand_vec_perm_d *d)
     {
       /* Selector sample: E_V8SImode, { 0, 1, 2, 3, 4, 5, 6, 7 }.  */
       if (d->perm[0] == 0)
-       {
-         for (i = 0; i < d->nelt / 2; i += 1)
-           {
-             remapped[i] = i;
-             remapped[i + d->nelt / 2] = i;
-           }
-       }
+	{
+	  for (i = 0; i < d->nelt / 2; i += 1)
+	    {
+	      remapped[i] = i;
+	      remapped[i + d->nelt / 2] = i;
+	    }
+	}
       else
-       {
-         /* { 8, 9, 10, 11, 12, 13, 14, 15 }.  */
-         for (i = 0; i < d->nelt / 2; i += 1)
-           {
-             idx = i + d->nelt / 2;
-             remapped[i] = idx;
-             remapped[i + d->nelt / 2] = idx;
-           }
-       }
+	{
+	  /* { 8, 9, 10, 11, 12, 13, 14, 15 }.  */
+	  for (i = 0; i < d->nelt / 2; i += 1)
+	    {
+	      idx = i + d->nelt / 2;
+	      remapped[i] = idx;
+	      remapped[i + d->nelt / 2] = idx;
+	    }
+	}
       /* Selector after: { 0, 1, 2, 3, 0, 1, 2, 3 }
-        { 8, 9, 10, 11, 8, 9, 10, 11 }  */
+	 { 8, 9, 10, 11, 8, 9, 10, 11 }  */
     }
   else if (loongarch_is_center_extraction (d))
     {
       /* sample: E_V4DImode, { 2, 3, 4, 5 }
-        In this condition, we can just copy high 128bit of op0 and low 128bit
-        of op1 to the target register by using xvpermi.q insn.  */
+	 In this condition, we can just copy high 128bit of op0 and low 128bit
+	 of op1 to the target register by using xvpermi.q insn.  */
       if (!d->testing_p)
-       {
-         emit_move_insn (d->target, d->op1);
-         switch (d->vmode)
-           {
-             case E_V4DImode:
-               emit_insn (gen_lasx_xvpermi_q_v4di (d->target, d->target,
-                                                   d->op0, GEN_INT (0x21)));
-               break;
-             case E_V4DFmode:
-               emit_insn (gen_lasx_xvpermi_q_v4df (d->target, d->target,
-                                                   d->op0, GEN_INT (0x21)));
-               break;
-             case E_V8SImode:
-               emit_insn (gen_lasx_xvpermi_q_v8si (d->target, d->target,
-                                                   d->op0, GEN_INT (0x21)));
-               break;
-             case E_V8SFmode:
-               emit_insn (gen_lasx_xvpermi_q_v8sf (d->target, d->target,
-                                                   d->op0, GEN_INT (0x21)));
-               break;
-             case E_V16HImode:
-               emit_insn (gen_lasx_xvpermi_q_v16hi (d->target, d->target,
-                                                    d->op0, GEN_INT (0x21)));
-               break;
-             case E_V32QImode:
-               emit_insn (gen_lasx_xvpermi_q_v32qi (d->target, d->target,
-                                                    d->op0, GEN_INT (0x21)));
-               break;
-             default:
-               break;
-           }
-       }
+	{
+	  emit_move_insn (d->target, d->op1);
+	  switch (d->vmode)
+	    {
+	      case E_V4DImode:
+		emit_insn (gen_lasx_xvpermi_q_v4di (d->target, d->target,
+						    d->op0, GEN_INT (0x21)));
+		break;
+	      case E_V4DFmode:
+		emit_insn (gen_lasx_xvpermi_q_v4df (d->target, d->target,
+						    d->op0, GEN_INT (0x21)));
+		break;
+	      case E_V8SImode:
+		emit_insn (gen_lasx_xvpermi_q_v8si (d->target, d->target,
+						    d->op0, GEN_INT (0x21)));
+		break;
+	      case E_V8SFmode:
+		emit_insn (gen_lasx_xvpermi_q_v8sf (d->target, d->target,
+						    d->op0, GEN_INT (0x21)));
+		break;
+	      case E_V16HImode:
+		emit_insn (gen_lasx_xvpermi_q_v16hi (d->target, d->target,
+						     d->op0, GEN_INT (0x21)));
+		break;
+	      case E_V32QImode:
+		emit_insn (gen_lasx_xvpermi_q_v32qi (d->target, d->target,
+						     d->op0, GEN_INT (0x21)));
+		break;
+	      default:
+		break;
+	    }
+	}
       ok = true;
       /* Finish the funtion directly.  */
       goto expand_perm_const_2_end;
@@ -9077,138 +9089,138 @@ loongarch_expand_vec_perm_const_2 (struct expand_vec_perm_d *d)
   else if (loongarch_is_reversing_permutation (d))
     {
       /* Selector sample: E_V8SImode, { 7, 6, 5, 4, 3, 2, 1, 0 }
-        one_vector_p == true  */
+	 one_vector_p == true  */
       idx = d->nelt / 2 - 1;
       for (i = 0; i < d->nelt / 2; i += 1)
-       {
-         remapped[i] = idx;
-         remapped[i + d->nelt / 2] = idx;
-         idx -= 1;
-       }
+	{
+	  remapped[i] = idx;
+	  remapped[i + d->nelt / 2] = idx;
+	  idx -= 1;
+	}
       /* Selector after: { 3, 2, 1, 0, 3, 2, 1, 0 }
-        Additional insn will be generated to swap hi and lo 128bit of target
-        register.  */
+	 Additional insn will be generated to swap hi and lo 128bit of target
+	 register.  */
       reverse_hi_lo = true;
     }
   else if (loongarch_is_di_misalign_extract (d)
-          || loongarch_is_si_misalign_extract (d))
+	   || loongarch_is_si_misalign_extract (d))
     {
       /* Selector Sample:
-        DI misalign: E_V4DImode, { 1, 2, 3, 4 }
-        SI misalign: E_V8SImode, { 1, 2, 3, 4, 5, 6, 7, 8 }  */
+	 DI misalign: E_V4DImode, { 1, 2, 3, 4 }
+	 SI misalign: E_V8SImode, { 1, 2, 3, 4, 5, 6, 7, 8 }  */
       if (!d->testing_p)
-       {
-         /* Copy original op0/op1 value to new temp register.
-            In some cases, operand register may be used in multiple place, so
-            we need new regiter instead modify original one, to avoid runtime
-            crashing or wrong value after execution.  */
-         use_alt_op = true;
-         op1_alt = gen_reg_rtx (d->vmode);
-         emit_move_insn (op1_alt, d->op1);
-
-         /* Adjust op1 for selecting correct value in high 128bit of target
-            register.
-            op1: E_V4DImode, { 4, 5, 6, 7 } -> { 2, 3, 4, 5 }.  */
-         rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
-         rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
-                                             conv_op0, GEN_INT (0x21)));
-
-         for (i = 0; i < d->nelt / 2; i += 1)
-           {
-             remapped[i] = d->perm[i];
-             remapped[i + d->nelt / 2] = d->perm[i];
-           }
-         /* Selector after:
-            DI misalign: { 1, 2, 1, 2 }
-            SI misalign: { 1, 2, 3, 4, 1, 2, 3, 4 }  */
-       }
+	{
+	  /* Copy original op0/op1 value to new temp register.
+	     In some cases, operand register may be used in multiple place, so
+	     we need new regiter instead modify original one, to avoid runtime
+	     crashing or wrong value after execution.  */
+	  use_alt_op = true;
+	  op1_alt = gen_reg_rtx (d->vmode);
+	  emit_move_insn (op1_alt, d->op1);
+
+	  /* Adjust op1 for selecting correct value in high 128bit of target
+	     register.
+	     op1: E_V4DImode, { 4, 5, 6, 7 } -> { 2, 3, 4, 5 }.  */
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
+					      conv_op0, GEN_INT (0x21)));
+
+	  for (i = 0; i < d->nelt / 2; i += 1)
+	    {
+	      remapped[i] = d->perm[i];
+	      remapped[i + d->nelt / 2] = d->perm[i];
+	    }
+	  /* Selector after:
+	     DI misalign: { 1, 2, 1, 2 }
+	     SI misalign: { 1, 2, 3, 4, 1, 2, 3, 4 }  */
+	}
     }
   else if (loongarch_is_lasx_lowpart_interleave (d))
     {
       /* Elements from op0's low 18bit and op1's 128bit are inserted into
-        target register alternately.
-        sample: E_V4DImode, { 0, 4, 1, 5 }  */
+	 target register alternately.
+	 sample: E_V4DImode, { 0, 4, 1, 5 }  */
       if (!d->testing_p)
-       {
-         /* Prepare temp register instead of modify original op.  */
-         use_alt_op = true;
-         op1_alt = gen_reg_rtx (d->vmode);
-         op0_alt = gen_reg_rtx (d->vmode);
-         emit_move_insn (op1_alt, d->op1);
-         emit_move_insn (op0_alt, d->op0);
-
-         /* Generate subreg for fitting into insn gen function.  */
-         rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
-         rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
-
-         /* Adjust op value in temp register.
-            op0 = {0,1,2,3}, op1 = {4,5,0,1}  */
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
-                                             conv_op0, GEN_INT (0x02)));
-         /* op0 = {0,1,4,5}, op1 = {4,5,0,1}  */
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
-                                             conv_op1, GEN_INT (0x01)));
-
-         /* Remap indices in selector based on the location of index inside
-            selector, and vector element numbers in current vector mode.  */
-
-         /* Filling low 128bit of new selector.  */
-         for (i = 0; i < d->nelt / 2; i += 1)
-           {
-             /* value in odd-indexed slot of low 128bit part of selector
-                vector.  */
-             remapped[i] = i % 2 != 0 ? d->perm[i] - d->nelt / 2 : d->perm[i];
-           }
-         /* Then filling the high 128bit.  */
-         for (i = d->nelt / 2; i < d->nelt; i += 1)
-           {
-             /* value in even-indexed slot of high 128bit part of
-                selector vector.  */
-             remapped[i] = i % 2 == 0
-               ? d->perm[i] + (d->nelt / 2) * 3 : d->perm[i];
-           }
-       }
+	{
+	  /* Prepare temp register instead of modify original op.  */
+	  use_alt_op = true;
+	  op1_alt = gen_reg_rtx (d->vmode);
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  emit_move_insn (op1_alt, d->op1);
+	  emit_move_insn (op0_alt, d->op0);
+
+	  /* Generate subreg for fitting into insn gen function.  */
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+
+	  /* Adjust op value in temp register.
+	     op0 = {0,1,2,3}, op1 = {4,5,0,1}  */
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
+					      conv_op0, GEN_INT (0x02)));
+	  /* op0 = {0,1,4,5}, op1 = {4,5,0,1}  */
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
+					      conv_op1, GEN_INT (0x01)));
+
+	  /* Remap indices in selector based on the location of index inside
+	     selector, and vector element numbers in current vector mode.  */
+
+	  /* Filling low 128bit of new selector.  */
+	  for (i = 0; i < d->nelt / 2; i += 1)
+	    {
+	      /* value in odd-indexed slot of low 128bit part of selector
+		 vector.  */
+	      remapped[i] = i % 2 != 0 ? d->perm[i] - d->nelt / 2 : d->perm[i];
+	    }
+	  /* Then filling the high 128bit.  */
+	  for (i = d->nelt / 2; i < d->nelt; i += 1)
+	    {
+	      /* value in even-indexed slot of high 128bit part of
+		 selector vector.  */
+	      remapped[i] = i % 2 == 0
+		? d->perm[i] + (d->nelt / 2) * 3 : d->perm[i];
+	    }
+	}
     }
   else if (loongarch_is_lasx_lowpart_interleave_2 (d))
     {
       /* Special lowpart interleave case in V32QI vector mode.  It does the same
-        thing as we can see in if branch that above this line.
-        Selector sample: E_V32QImode,
-        {0, 1, 2, 3, 4, 5, 6, 7, 32, 33, 34, 35, 36, 37, 38, 39, 8,
-        9, 10, 11, 12, 13, 14, 15, 40, 41, 42, 43, 44, 45, 46, 47}  */
+	 thing as we can see in if branch that above this line.
+	 Selector sample: E_V32QImode,
+	 {0, 1, 2, 3, 4, 5, 6, 7, 32, 33, 34, 35, 36, 37, 38, 39, 8,
+	 9, 10, 11, 12, 13, 14, 15, 40, 41, 42, 43, 44, 45, 46, 47}  */
       if (!d->testing_p)
-       {
-         /* Solution for this case in very simple - covert op into V4DI mode,
-            and do same thing as previous if branch.  */
-         op1_alt = gen_reg_rtx (d->vmode);
-         op0_alt = gen_reg_rtx (d->vmode);
-         emit_move_insn (op1_alt, d->op1);
-         emit_move_insn (op0_alt, d->op0);
-
-         rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
-         rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
-         rtx conv_target = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
-
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
-                                             conv_op0, GEN_INT (0x02)));
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
-                                             conv_op1, GEN_INT (0x01)));
-         remapped[0] = 0;
-         remapped[1] = 4;
-         remapped[2] = 1;
-         remapped[3] = 5;
-
-         for (i = 0; i < d->nelt; i += 1)
-           {
-             rperm[i] = GEN_INT (remapped[i]);
-           }
-
-         sel = gen_rtx_CONST_VECTOR (E_V4DImode, gen_rtvec_v (4, rperm));
-         sel = force_reg (E_V4DImode, sel);
-         emit_insn (gen_lasx_xvshuf_d (conv_target, sel,
-                                       conv_op1, conv_op0));
-       }
+	{
+	  /* Solution for this case in very simple - covert op into V4DI mode,
+	     and do same thing as previous if branch.  */
+	  op1_alt = gen_reg_rtx (d->vmode);
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  emit_move_insn (op1_alt, d->op1);
+	  emit_move_insn (op0_alt, d->op0);
+
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+	  rtx conv_target = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
+
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
+					      conv_op0, GEN_INT (0x02)));
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
+					      conv_op1, GEN_INT (0x01)));
+	  remapped[0] = 0;
+	  remapped[1] = 4;
+	  remapped[2] = 1;
+	  remapped[3] = 5;
+
+	  for (i = 0; i < d->nelt; i += 1)
+	    {
+	      rperm[i] = GEN_INT (remapped[i]);
+	    }
+
+	  sel = gen_rtx_CONST_VECTOR (E_V4DImode, gen_rtvec_v (4, rperm));
+	  sel = force_reg (E_V4DImode, sel);
+	  emit_insn (gen_lasx_xvshuf_d (conv_target, sel,
+					conv_op1, conv_op0));
+	}
 
       ok = true;
       goto expand_perm_const_2_end;
@@ -9216,19 +9228,19 @@ loongarch_expand_vec_perm_const_2 (struct expand_vec_perm_d *d)
   else if (loongarch_is_lasx_lowpart_extract (d))
     {
       /* Copy op0's low 128bit to target's low 128bit, and copy op1's low
-        128bit to target's high 128bit.
-        Selector sample: E_V4DImode, { 0, 1, 4 ,5 }  */
+	 128bit to target's high 128bit.
+	 Selector sample: E_V4DImode, { 0, 1, 4 ,5 }  */
       if (!d->testing_p)
-       {
-         rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, d->op1, 0);
-         rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
-         rtx conv_target = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
-
-         /* We can achieve the expectation by using sinple xvpermi.q insn.  */
-         emit_move_insn (conv_target, conv_op1);
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_target, conv_target,
-                                             conv_op0, GEN_INT (0x20)));
-       }
+	{
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, d->op1, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
+	  rtx conv_target = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
+
+	  /* We can achieve the expectation by using sinple xvpermi.q insn.  */
+	  emit_move_insn (conv_target, conv_op1);
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_target, conv_target,
+					      conv_op0, GEN_INT (0x20)));
+	}
 
       ok = true;
       goto expand_perm_const_2_end;
@@ -9236,339 +9248,339 @@ loongarch_expand_vec_perm_const_2 (struct expand_vec_perm_d *d)
   else if (loongarch_is_lasx_highpart_interleave (d))
     {
       /* Similar to lowpart interleave, elements from op0's high 128bit and
-        op1's high 128bit are inserted into target regiter alternately.
-        Selector sample: E_V8SImode, { 4, 12, 5, 13, 6, 14, 7, 15 }  */
+	 op1's high 128bit are inserted into target regiter alternately.
+	 Selector sample: E_V8SImode, { 4, 12, 5, 13, 6, 14, 7, 15 }  */
       if (!d->testing_p)
-       {
-         /* Prepare temp op register.  */
-         use_alt_op = true;
-         op1_alt = gen_reg_rtx (d->vmode);
-         op0_alt = gen_reg_rtx (d->vmode);
-         emit_move_insn (op1_alt, d->op1);
-         emit_move_insn (op0_alt, d->op0);
-
-         rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
-         rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
-         /* Adjust op value in temp regiter.
-            op0 = { 0, 1, 2, 3 }, op1 = { 6, 7, 2, 3 }  */
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
-                                             conv_op0, GEN_INT (0x13)));
-         /* op0 = { 2, 3, 6, 7 }, op1 = { 6, 7, 2, 3 }  */
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
-                                             conv_op1, GEN_INT (0x01)));
-         /* Remap indices in selector based on the location of index inside
-            selector, and vector element numbers in current vector mode.  */
-
-         /* Filling low 128bit of new selector.  */
-        for (i = 0; i < d->nelt / 2; i += 1)
-          {
-            /* value in even-indexed slot of low 128bit part of selector
-               vector.  */
-            remapped[i] = i % 2 == 0 ? d->perm[i] - d->nelt / 2 : d->perm[i];
-          }
-         /* Then filling the high 128bit.  */
-        for (i = d->nelt / 2; i < d->nelt; i += 1)
-          {
-            /* value in odd-indexed slot of high 128bit part of selector
-               vector.  */
-             remapped[i] = i % 2 != 0
-               ? d->perm[i] - (d->nelt / 2) * 3 : d->perm[i];
-          }
-       }
+	{
+	  /* Prepare temp op register.  */
+	  use_alt_op = true;
+	  op1_alt = gen_reg_rtx (d->vmode);
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  emit_move_insn (op1_alt, d->op1);
+	  emit_move_insn (op0_alt, d->op0);
+
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+	  /* Adjust op value in temp regiter.
+	     op0 = { 0, 1, 2, 3 }, op1 = { 6, 7, 2, 3 }  */
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
+					      conv_op0, GEN_INT (0x13)));
+	  /* op0 = { 2, 3, 6, 7 }, op1 = { 6, 7, 2, 3 }  */
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
+					      conv_op1, GEN_INT (0x01)));
+	  /* Remap indices in selector based on the location of index inside
+	     selector, and vector element numbers in current vector mode.  */
+
+	  /* Filling low 128bit of new selector.  */
+	 for (i = 0; i < d->nelt / 2; i += 1)
+	   {
+	     /* value in even-indexed slot of low 128bit part of selector
+		vector.  */
+	     remapped[i] = i % 2 == 0 ? d->perm[i] - d->nelt / 2 : d->perm[i];
+	   }
+	  /* Then filling the high 128bit.  */
+	 for (i = d->nelt / 2; i < d->nelt; i += 1)
+	   {
+	     /* value in odd-indexed slot of high 128bit part of selector
+		vector.  */
+	      remapped[i] = i % 2 != 0
+		? d->perm[i] - (d->nelt / 2) * 3 : d->perm[i];
+	   }
+	}
     }
   else if (loongarch_is_lasx_highpart_interleave_2 (d))
     {
       /* Special highpart interleave case in V32QI vector mode.  It does the
-        same thing as the normal version above.
-        Selector sample: E_V32QImode,
-        {16, 17, 18, 19, 20, 21, 22, 23, 48, 49, 50, 51, 52, 53, 54, 55,
-        24, 25, 26, 27, 28, 29, 30, 31, 56, 57, 58, 59, 60, 61, 62, 63}
+	 same thing as the normal version above.
+	 Selector sample: E_V32QImode,
+	 {16, 17, 18, 19, 20, 21, 22, 23, 48, 49, 50, 51, 52, 53, 54, 55,
+	 24, 25, 26, 27, 28, 29, 30, 31, 56, 57, 58, 59, 60, 61, 62, 63}
       */
       if (!d->testing_p)
-       {
-         /* Convert op into V4DImode and do the things.  */
-         op1_alt = gen_reg_rtx (d->vmode);
-         op0_alt = gen_reg_rtx (d->vmode);
-         emit_move_insn (op1_alt, d->op1);
-         emit_move_insn (op0_alt, d->op0);
-
-         rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
-         rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
-         rtx conv_target = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
-
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
-                                             conv_op0, GEN_INT (0x13)));
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
-                                             conv_op1, GEN_INT (0x01)));
-         remapped[0] = 2;
-         remapped[1] = 6;
-         remapped[2] = 3;
-         remapped[3] = 7;
-
-         for (i = 0; i < d->nelt; i += 1)
-           {
-             rperm[i] = GEN_INT (remapped[i]);
-           }
-
-         sel = gen_rtx_CONST_VECTOR (E_V4DImode, gen_rtvec_v (4, rperm));
-         sel = force_reg (E_V4DImode, sel);
-         emit_insn (gen_lasx_xvshuf_d (conv_target, sel,
-                                       conv_op1, conv_op0));
-       }
-
-       ok = true;
-       goto expand_perm_const_2_end;
+	{
+	  /* Convert op into V4DImode and do the things.  */
+	  op1_alt = gen_reg_rtx (d->vmode);
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  emit_move_insn (op1_alt, d->op1);
+	  emit_move_insn (op0_alt, d->op0);
+
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+	  rtx conv_target = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
+
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
+					      conv_op0, GEN_INT (0x13)));
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
+					      conv_op1, GEN_INT (0x01)));
+	  remapped[0] = 2;
+	  remapped[1] = 6;
+	  remapped[2] = 3;
+	  remapped[3] = 7;
+
+	  for (i = 0; i < d->nelt; i += 1)
+	    {
+	      rperm[i] = GEN_INT (remapped[i]);
+	    }
+
+	  sel = gen_rtx_CONST_VECTOR (E_V4DImode, gen_rtvec_v (4, rperm));
+	  sel = force_reg (E_V4DImode, sel);
+	  emit_insn (gen_lasx_xvshuf_d (conv_target, sel,
+					conv_op1, conv_op0));
+	}
+
+	ok = true;
+	goto expand_perm_const_2_end;
     }
   else if (loongarch_is_elem_duplicate (d))
     {
       /* Brocast single element (from op0 or op1) to all slot of target
-        register.
-        Selector sample:E_V8SImode, { 2, 2, 2, 2, 2, 2, 2, 2 }  */
+	 register.
+	 Selector sample:E_V8SImode, { 2, 2, 2, 2, 2, 2, 2, 2 }  */
       if (!d->testing_p)
-       {
-         rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, d->op1, 0);
-         rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
-         rtx temp_reg = gen_reg_rtx (d->vmode);
-         rtx conv_temp = gen_rtx_SUBREG (E_V4DImode, temp_reg, 0);
-
-         emit_move_insn (temp_reg, d->op0);
-
-         idx = d->perm[0];
-         /* We will use xvrepl128vei.* insn to achieve the result, but we need
-            to make the high/low 128bit has the same contents that contain the
-            value that we need to broardcast, because xvrepl128vei does the
-            broardcast job from every 128bit of source register to
-            corresponded part of target register! (A deep sigh.)  */
-         if (/*idx >= 0 &&*/ idx < d->nelt / 2)
-           {
-             emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
-                                                 conv_op0, GEN_INT (0x0)));
-           }
-         else if (idx >= d->nelt / 2 && idx < d->nelt)
-           {
-             emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
-                                                 conv_op0, GEN_INT (0x11)));
-             idx -= d->nelt / 2;
-           }
-         else if (idx >= d->nelt && idx < (d->nelt + d->nelt / 2))
-           {
-             emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
-                                                 conv_op1, GEN_INT (0x0)));
-           }
-         else if (idx >= (d->nelt + d->nelt / 2) && idx < d->nelt * 2)
-           {
-             emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
-                                                 conv_op1, GEN_INT (0x11)));
-             idx -= d->nelt / 2;
-           }
-
-         /* Then we can finally generate this insn.  */
-         switch (d->vmode)
-           {
-           case E_V4DImode:
-             emit_insn (gen_lasx_xvrepl128vei_d (d->target, temp_reg,
-                                                 GEN_INT (idx)));
-             break;
-           case E_V4DFmode:
-             emit_insn (gen_lasx_xvrepl128vei_d_f (d->target, temp_reg,
-                                                   GEN_INT (idx)));
-             break;
-           case E_V8SImode:
-             emit_insn (gen_lasx_xvrepl128vei_w (d->target, temp_reg,
-                                                 GEN_INT (idx)));
-             break;
-           case E_V8SFmode:
-             emit_insn (gen_lasx_xvrepl128vei_w_f (d->target, temp_reg,
-                                                   GEN_INT (idx)));
-             break;
-           case E_V16HImode:
-             emit_insn (gen_lasx_xvrepl128vei_h (d->target, temp_reg,
-                                                 GEN_INT (idx)));
-             break;
-           case E_V32QImode:
-             emit_insn (gen_lasx_xvrepl128vei_b (d->target, temp_reg,
-                                                 GEN_INT (idx)));
-             break;
-           default:
-             gcc_unreachable ();
-             break;
-           }
-
-         /* finish func directly.  */
-         ok = true;
-         goto expand_perm_const_2_end;
-       }
+	{
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, d->op1, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
+	  rtx temp_reg = gen_reg_rtx (d->vmode);
+	  rtx conv_temp = gen_rtx_SUBREG (E_V4DImode, temp_reg, 0);
+
+	  emit_move_insn (temp_reg, d->op0);
+
+	  idx = d->perm[0];
+	  /* We will use xvrepl128vei.* insn to achieve the result, but we need
+	     to make the high/low 128bit has the same contents that contain the
+	     value that we need to broardcast, because xvrepl128vei does the
+	     broardcast job from every 128bit of source register to
+	     corresponded part of target register! (A deep sigh.)  */
+	  if (/*idx >= 0 &&*/ idx < d->nelt / 2)
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
+						  conv_op0, GEN_INT (0x0)));
+	    }
+	  else if (idx >= d->nelt / 2 && idx < d->nelt)
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
+						  conv_op0, GEN_INT (0x11)));
+	      idx -= d->nelt / 2;
+	    }
+	  else if (idx >= d->nelt && idx < (d->nelt + d->nelt / 2))
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
+						  conv_op1, GEN_INT (0x0)));
+	    }
+	  else if (idx >= (d->nelt + d->nelt / 2) && idx < d->nelt * 2)
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
+						  conv_op1, GEN_INT (0x11)));
+	      idx -= d->nelt / 2;
+	    }
+
+	  /* Then we can finally generate this insn.  */
+	  switch (d->vmode)
+	    {
+	    case E_V4DImode:
+	      emit_insn (gen_lasx_xvrepl128vei_d (d->target, temp_reg,
+						  GEN_INT (idx)));
+	      break;
+	    case E_V4DFmode:
+	      emit_insn (gen_lasx_xvrepl128vei_d_f (d->target, temp_reg,
+						    GEN_INT (idx)));
+	      break;
+	    case E_V8SImode:
+	      emit_insn (gen_lasx_xvrepl128vei_w (d->target, temp_reg,
+						  GEN_INT (idx)));
+	      break;
+	    case E_V8SFmode:
+	      emit_insn (gen_lasx_xvrepl128vei_w_f (d->target, temp_reg,
+						    GEN_INT (idx)));
+	      break;
+	    case E_V16HImode:
+	      emit_insn (gen_lasx_xvrepl128vei_h (d->target, temp_reg,
+						  GEN_INT (idx)));
+	      break;
+	    case E_V32QImode:
+	      emit_insn (gen_lasx_xvrepl128vei_b (d->target, temp_reg,
+						  GEN_INT (idx)));
+	      break;
+	    default:
+	      gcc_unreachable ();
+	      break;
+	    }
+
+	  /* finish func directly.  */
+	  ok = true;
+	  goto expand_perm_const_2_end;
+	}
     }
   else if (loongarch_is_op_reverse_perm (d))
     {
       /* reverse high 128bit and low 128bit in op0.
-        Selector sample: E_V4DFmode, { 2, 3, 0, 1 }
-        Use xvpermi.q for doing this job.  */
-      if (!d->testing_p)
-       {
-         if (d->vmode == E_V4DImode)
-           {
-             emit_insn (gen_lasx_xvpermi_q_v4di (d->target, d->target, d->op0,
-                                                 GEN_INT (0x01)));
-           }
-         else if (d->vmode == E_V4DFmode)
-           {
-             emit_insn (gen_lasx_xvpermi_q_v4df (d->target, d->target, d->op0,
-                                                 GEN_INT (0x01)));
-           }
-         else
-           {
-             gcc_unreachable ();
-           }
-       }
-
-      ok = true;
-      goto expand_perm_const_2_end;
-    }
-  else if (loongarch_is_single_op_perm (d))
-    {
-      /* Permutation that only select elements from op0.  */
+	 Selector sample: E_V4DFmode, { 2, 3, 0, 1 }
+	 Use xvpermi.q for doing this job.  */
       if (!d->testing_p)
-       {
-         /* Prepare temp register instead of modify original op.  */
-         use_alt_op = true;
-         op0_alt = gen_reg_rtx (d->vmode);
-         op1_alt = gen_reg_rtx (d->vmode);
-
-         emit_move_insn (op0_alt, d->op0);
-         emit_move_insn (op1_alt, d->op1);
-
-         rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
-         rtx conv_op0a = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
-         rtx conv_op1a = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
-
-         /* Duplicate op0's low 128bit in op0, then duplicate high 128bit
-            in op1.  After this, xvshuf.* insn's selector argument can
-            access all elements we need for correct permutation result.  */
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0a, conv_op0a, conv_op0,
-                                             GEN_INT (0x00)));
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1a, conv_op1a, conv_op0,
-                                             GEN_INT (0x11)));
-
-         /* In this case, there's no need to remap selector's indices.  */
-         for (i = 0; i < d->nelt; i += 1)
-           {
-             remapped[i] = d->perm[i];
-           }
-       }
+	{
+	  if (d->vmode == E_V4DImode)
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4di (d->target, d->target, d->op0,
+						  GEN_INT (0x01)));
+	    }
+	  else if (d->vmode == E_V4DFmode)
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4df (d->target, d->target, d->op0,
+						  GEN_INT (0x01)));
+	    }
+	  else
+	    {
+	      gcc_unreachable ();
+	    }
+	}
+
+      ok = true;
+      goto expand_perm_const_2_end;
+    }
+  else if (loongarch_is_single_op_perm (d))
+    {
+      /* Permutation that only select elements from op0.  */
+      if (!d->testing_p)
+	{
+	  /* Prepare temp register instead of modify original op.  */
+	  use_alt_op = true;
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  op1_alt = gen_reg_rtx (d->vmode);
+
+	  emit_move_insn (op0_alt, d->op0);
+	  emit_move_insn (op1_alt, d->op1);
+
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
+	  rtx conv_op0a = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+	  rtx conv_op1a = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+
+	  /* Duplicate op0's low 128bit in op0, then duplicate high 128bit
+	     in op1.  After this, xvshuf.* insn's selector argument can
+	     access all elements we need for correct permutation result.  */
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0a, conv_op0a, conv_op0,
+					      GEN_INT (0x00)));
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1a, conv_op1a, conv_op0,
+					      GEN_INT (0x11)));
+
+	  /* In this case, there's no need to remap selector's indices.  */
+	  for (i = 0; i < d->nelt; i += 1)
+	    {
+	      remapped[i] = d->perm[i];
+	    }
+	}
     }
   else if (loongarch_is_divisible_perm (d))
     {
       /* Divisible perm:
-        Low 128bit of selector only selects elements of op0,
-        and high 128bit of selector only selects elements of op1.  */
+	 Low 128bit of selector only selects elements of op0,
+	 and high 128bit of selector only selects elements of op1.  */
 
       if (!d->testing_p)
-       {
-         /* Prepare temp register instead of modify original op.  */
-         use_alt_op = true;
-         op0_alt = gen_reg_rtx (d->vmode);
-         op1_alt = gen_reg_rtx (d->vmode);
-
-         emit_move_insn (op0_alt, d->op0);
-         emit_move_insn (op1_alt, d->op1);
-
-         rtx conv_op0a = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
-         rtx conv_op1a = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
-         rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
-         rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, d->op1, 0);
-
-         /* Reorganize op0's hi/lo 128bit and op1's hi/lo 128bit, to make sure
-            that selector's low 128bit can access all op0's elements, and
-            selector's high 128bit can access all op1's elements.  */
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0a, conv_op0a, conv_op1,
-                                             GEN_INT (0x02)));
-         emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1a, conv_op1a, conv_op0,
-                                             GEN_INT (0x31)));
-
-         /* No need to modify indices.  */
-         for (i = 0; i < d->nelt;i += 1)
-           {
-             remapped[i] = d->perm[i];
-           }
-       }
+	{
+	  /* Prepare temp register instead of modify original op.  */
+	  use_alt_op = true;
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  op1_alt = gen_reg_rtx (d->vmode);
+
+	  emit_move_insn (op0_alt, d->op0);
+	  emit_move_insn (op1_alt, d->op1);
+
+	  rtx conv_op0a = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+	  rtx conv_op1a = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, d->op1, 0);
+
+	  /* Reorganize op0's hi/lo 128bit and op1's hi/lo 128bit, to make sure
+	     that selector's low 128bit can access all op0's elements, and
+	     selector's high 128bit can access all op1's elements.  */
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0a, conv_op0a, conv_op1,
+					      GEN_INT (0x02)));
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1a, conv_op1a, conv_op0,
+					      GEN_INT (0x31)));
+
+	  /* No need to modify indices.  */
+	  for (i = 0; i < d->nelt;i += 1)
+	    {
+	      remapped[i] = d->perm[i];
+	    }
+	}
     }
   else if (loongarch_is_triple_stride_extract (d))
     {
       /* Selector sample: E_V4DFmode, { 1, 4, 7, 0 }.  */
       if (!d->testing_p)
-       {
-         /* Resolve it with brute force modification.  */
-         remapped[0] = 1;
-         remapped[1] = 2;
-         remapped[2] = 3;
-         remapped[3] = 0;
-       }
+	{
+	  /* Resolve it with brute force modification.  */
+	  remapped[0] = 1;
+	  remapped[1] = 2;
+	  remapped[2] = 3;
+	  remapped[3] = 0;
+	}
     }
   else
     {
       /* When all of the detections above are failed, we will try last
-        strategy.
-        The for loop tries to detect following rules based on indices' value,
-        its position inside of selector vector ,and strange behavior of
-        xvshuf.* insn; Then we take corresponding action. (Replace with new
-        value, or give up whole permutation expansion.)  */
+	 strategy.
+	 The for loop tries to detect following rules based on indices' value,
+	 its position inside of selector vector ,and strange behavior of
+	 xvshuf.* insn; Then we take corresponding action. (Replace with new
+	 value, or give up whole permutation expansion.)  */
       for (i = 0; i < d->nelt; i += 1)
-       {
-         /* % (2 * d->nelt)  */
-         idx = d->perm[i];
-
-         /* if index is located in low 128bit of selector vector.  */
-         if (i < d->nelt / 2)
-           {
-             /* Fail case 1: index tries to reach element that located in op0's
-                high 128bit.  */
-             if (idx >= d->nelt / 2 && idx < d->nelt)
-               {
-                 goto expand_perm_const_2_end;
-               }
-             /* Fail case 2: index tries to reach element that located in
-                op1's high 128bit.  */
-             if (idx >= (d->nelt + d->nelt / 2))
-               {
-                 goto expand_perm_const_2_end;
-               }
-
-             /* Success case: index tries to reach elements that located in
-                op1's low 128bit.  Apply - (nelt / 2) offset to original
-                value.  */
-             if (idx >= d->nelt && idx < (d->nelt + d->nelt / 2))
-               {
-                 idx -= d->nelt / 2;
-               }
-           }
-         /* if index is located in high 128bit of selector vector.  */
-         else
-           {
-             /* Fail case 1: index tries to reach element that located in
-                op1's low 128bit.  */
-             if (idx >= d->nelt && idx < (d->nelt + d->nelt / 2))
-               {
-                 goto expand_perm_const_2_end;
-               }
-             /* Fail case 2: index tries to reach element that located in
-                op0's low 128bit.  */
-             if (idx < (d->nelt / 2))
-               {
-                 goto expand_perm_const_2_end;
-               }
-             /* Success case: index tries to reach element that located in
-                op0's high 128bit.  */
-             if (idx >= d->nelt / 2 && idx < d->nelt)
-               {
-                 idx -= d->nelt / 2;
-               }
-           }
-         /* No need to process other case that we did not mentioned.  */
-
-         /* Assign with original or processed value.  */
-         remapped[i] = idx;
-       }
+	{
+	  /* % (2 * d->nelt)  */
+	  idx = d->perm[i];
+
+	  /* if index is located in low 128bit of selector vector.  */
+	  if (i < d->nelt / 2)
+	    {
+	      /* Fail case 1: index tries to reach element that located in op0's
+		 high 128bit.  */
+	      if (idx >= d->nelt / 2 && idx < d->nelt)
+		{
+		  goto expand_perm_const_2_end;
+		}
+	      /* Fail case 2: index tries to reach element that located in
+		 op1's high 128bit.  */
+	      if (idx >= (d->nelt + d->nelt / 2))
+		{
+		  goto expand_perm_const_2_end;
+		}
+
+	      /* Success case: index tries to reach elements that located in
+		 op1's low 128bit.  Apply - (nelt / 2) offset to original
+		 value.  */
+	      if (idx >= d->nelt && idx < (d->nelt + d->nelt / 2))
+		{
+		  idx -= d->nelt / 2;
+		}
+	    }
+	  /* if index is located in high 128bit of selector vector.  */
+	  else
+	    {
+	      /* Fail case 1: index tries to reach element that located in
+		 op1's low 128bit.  */
+	      if (idx >= d->nelt && idx < (d->nelt + d->nelt / 2))
+		{
+		  goto expand_perm_const_2_end;
+		}
+	      /* Fail case 2: index tries to reach element that located in
+		 op0's low 128bit.  */
+	      if (idx < (d->nelt / 2))
+		{
+		  goto expand_perm_const_2_end;
+		}
+	      /* Success case: index tries to reach element that located in
+		 op0's high 128bit.  */
+	      if (idx >= d->nelt / 2 && idx < d->nelt)
+		{
+		  idx -= d->nelt / 2;
+		}
+	    }
+	  /* No need to process other case that we did not mentioned.  */
+
+	  /* Assign with original or processed value.  */
+	  remapped[i] = idx;
+	}
     }
 
   ok = true;
@@ -9654,34 +9666,34 @@ loongarch_expand_vec_perm_const_2 (struct expand_vec_perm_d *d)
   if (reverse_hi_lo)
     {
       switch (d->vmode)
-       {
-       case E_V4DFmode:
-         emit_insn (gen_lasx_xvpermi_q_v4df (d->target, d->target,
-                                             d->target, GEN_INT (0x1)));
-         break;
-       case E_V4DImode:
-         emit_insn (gen_lasx_xvpermi_q_v4di (d->target, d->target,
-                                             d->target, GEN_INT (0x1)));
-         break;
-       case E_V8SFmode:
-         emit_insn (gen_lasx_xvpermi_q_v8sf (d->target, d->target,
-                                             d->target, GEN_INT (0x1)));
-         break;
-       case E_V8SImode:
-         emit_insn (gen_lasx_xvpermi_q_v8si (d->target, d->target,
-                                             d->target, GEN_INT (0x1)));
-         break;
-       case E_V16HImode:
-         emit_insn (gen_lasx_xvpermi_q_v16hi (d->target, d->target,
-                                              d->target, GEN_INT (0x1)));
-         break;
-       case E_V32QImode:
-         emit_insn (gen_lasx_xvpermi_q_v32qi (d->target, d->target,
-                                              d->target, GEN_INT (0x1)));
-         break;
-       default:
-         break;
-       }
+	{
+	case E_V4DFmode:
+	  emit_insn (gen_lasx_xvpermi_q_v4df (d->target, d->target,
+					      d->target, GEN_INT (0x1)));
+	  break;
+	case E_V4DImode:
+	  emit_insn (gen_lasx_xvpermi_q_v4di (d->target, d->target,
+					      d->target, GEN_INT (0x1)));
+	  break;
+	case E_V8SFmode:
+	  emit_insn (gen_lasx_xvpermi_q_v8sf (d->target, d->target,
+					      d->target, GEN_INT (0x1)));
+	  break;
+	case E_V8SImode:
+	  emit_insn (gen_lasx_xvpermi_q_v8si (d->target, d->target,
+					      d->target, GEN_INT (0x1)));
+	  break;
+	case E_V16HImode:
+	  emit_insn (gen_lasx_xvpermi_q_v16hi (d->target, d->target,
+					       d->target, GEN_INT (0x1)));
+	  break;
+	case E_V32QImode:
+	  emit_insn (gen_lasx_xvpermi_q_v32qi (d->target, d->target,
+					       d->target, GEN_INT (0x1)));
+	  break;
+	default:
+	  break;
+	}
     }
   /* Extra insn required by odd/even extraction.  Swapping the second and third
      64bit in target vector register.  */
@@ -9689,7 +9701,7 @@ loongarch_expand_vec_perm_const_2 (struct expand_vec_perm_d *d)
     {
       rtx converted = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
       emit_insn (gen_lasx_xvpermi_d_v4di (converted, converted,
-                                         GEN_INT (0xD8)));
+					  GEN_INT (0xD8)));
     }
 
 expand_perm_const_2_end:
@@ -9700,8 +9712,8 @@ expand_perm_const_2_end:
 
 static bool
 loongarch_vectorize_vec_perm_const (machine_mode vmode,
-           rtx target, rtx op0, rtx op1,
-           const vec_perm_indices &sel)
+				    rtx target, rtx op0, rtx op1,
+				    const vec_perm_indices &sel)
 {
   struct expand_vec_perm_d d;
   int i, nelt, which;
@@ -9713,7 +9725,7 @@ loongarch_vectorize_vec_perm_const (machine_mode vmode,
     {
       rtx nop0 = force_reg (vmode, op0);
       if (op0 == op1)
- op1 = nop0;
+	op1 = nop0;
       op0 = nop0;
     }
   if (op1)
@@ -9745,12 +9757,12 @@ loongarch_vectorize_vec_perm_const (machine_mode vmode,
     case 3:
       d.one_vector_p = false;
       if (d.testing_p || !rtx_equal_p (d.op0, d.op1))
- break;
+	break;
       /* FALLTHRU */
 
     case 2:
       for (i = 0; i < nelt; ++i)
- d.perm[i] &= nelt - 1;
+	d.perm[i] &= nelt - 1;
       d.op0 = d.op1;
       d.one_vector_p = true;
       break;
@@ -9772,11 +9784,11 @@ loongarch_vectorize_vec_perm_const (machine_mode vmode,
       d.target = gen_raw_REG (d.vmode, LAST_VIRTUAL_REGISTER + 1);
       d.op1 = d.op0 = gen_raw_REG (d.vmode, LAST_VIRTUAL_REGISTER + 2);
       if (!d.one_vector_p)
- d.op1 = gen_raw_REG (d.vmode, LAST_VIRTUAL_REGISTER + 3);
+	d.op1 = gen_raw_REG (d.vmode, LAST_VIRTUAL_REGISTER + 3);
 
       ok = loongarch_expand_vec_perm_const_2 (&d);
       if (ok)
- return ok;
+	return ok;
 
       start_sequence ();
       ok = loongarch_expand_vec_perm_const_1 (&d);
@@ -9809,7 +9821,7 @@ loongarch_vectorize_vec_perm_const (machine_mode vmode,
 
 static int
 loongarch_cpu_sched_reassociation_width (struct loongarch_target *target,
-          unsigned int opc, machine_mode mode)
+					 unsigned int opc, machine_mode mode)
 {
   /* unreferenced argument */
   (void) opc;
@@ -9821,23 +9833,23 @@ loongarch_cpu_sched_reassociation_width (struct loongarch_target *target,
     case CPU_LA664:
       /* Vector part.  */
       if (LSX_SUPPORTED_MODE_P (mode) || LASX_SUPPORTED_MODE_P (mode))
- {
-   /* Integer vector instructions execute in FP unit.
-      The width of integer/float-point vector instructions is 3.  */
-   return 3;
- }
+	{
+	  /* Integer vector instructions execute in FP unit.
+	     The width of integer/float-point vector instructions is 3.  */
+	  return 3;
+	}
 
       /* Scalar part.  */
       else if (INTEGRAL_MODE_P (mode))
- return 1;
+	return 1;
       else if (FLOAT_MODE_P (mode))
- {
-   if (opc == PLUS_EXPR)
-     {
-       return 2;
-     }
-   return 4;
- }
+	{
+	  if (opc == PLUS_EXPR)
+	    {
+	      return 2;
+	    }
+	  return 4;
+	}
       break;
     default:
       break;
@@ -10003,9 +10015,9 @@ loongarch_expand_vector_reduc (rtx (*fn) (rtx, rtx, rtx), rtx dest, rtx in)
       half = gen_reg_rtx (mode);
       emit_reduc_half (half, vec, i);
       if (i == GET_MODE_UNIT_BITSIZE (mode) * 2)
- dst = dest;
+	dst = dest;
       else
- dst = gen_reg_rtx (mode);
+	dst = gen_reg_rtx (mode);
       emit_insn (fn (dst, half, vec));
       vec = dst;
     }
@@ -10026,93 +10038,93 @@ loongarch_expand_vec_unpack (rtx operands[2], bool unsigned_p, bool high_p)
   if (ISA_HAS_LASX && GET_MODE_SIZE (imode) == 32)
     {
       switch (imode)
- {
- case E_V8SImode:
-   if (unsigned_p)
-     extend = gen_lasx_vext2xv_du_wu;
-   else
-     extend = gen_lasx_vext2xv_d_w;
-   swap_hi_lo = gen_lasx_xvpermi_q_v8si;
-   break;
-
- case E_V16HImode:
-   if (unsigned_p)
-     extend = gen_lasx_vext2xv_wu_hu;
-   else
-     extend = gen_lasx_vext2xv_w_h;
-   swap_hi_lo = gen_lasx_xvpermi_q_v16hi;
-   break;
-
- case E_V32QImode:
-   if (unsigned_p)
-     extend = gen_lasx_vext2xv_hu_bu;
-   else
-     extend = gen_lasx_vext2xv_h_b;
-   swap_hi_lo = gen_lasx_xvpermi_q_v32qi;
-   break;
-
- default:
-   gcc_unreachable ();
-   break;
- }
+	{
+	case E_V8SImode:
+	  if (unsigned_p)
+	    extend = gen_lasx_vext2xv_du_wu;
+	  else
+	    extend = gen_lasx_vext2xv_d_w;
+	  swap_hi_lo = gen_lasx_xvpermi_q_v8si;
+	  break;
+
+	case E_V16HImode:
+	  if (unsigned_p)
+	    extend = gen_lasx_vext2xv_wu_hu;
+	  else
+	    extend = gen_lasx_vext2xv_w_h;
+	  swap_hi_lo = gen_lasx_xvpermi_q_v16hi;
+	  break;
+
+	case E_V32QImode:
+	  if (unsigned_p)
+	    extend = gen_lasx_vext2xv_hu_bu;
+	  else
+	    extend = gen_lasx_vext2xv_h_b;
+	  swap_hi_lo = gen_lasx_xvpermi_q_v32qi;
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	  break;
+	}
 
       if (high_p)
- {
-   tmp = gen_reg_rtx (imode);
-   emit_insn (swap_hi_lo (tmp, tmp, operands[1], const1_rtx));
-   emit_insn (extend (operands[0], tmp));
-   return;
- }
+	{
+	  tmp = gen_reg_rtx (imode);
+	  emit_insn (swap_hi_lo (tmp, tmp, operands[1], const1_rtx));
+	  emit_insn (extend (operands[0], tmp));
+	  return;
+	}
 
       emit_insn (extend (operands[0], operands[1]));
       return;
 
     }
-
   else if (ISA_HAS_LSX)
     {
       switch (imode)
- {
- case E_V4SImode:
-   if (high_p != 0)
-     unpack = gen_lsx_vilvh_w;
-   else
-     unpack = gen_lsx_vilvl_w;
-
-   cmpFunc = gen_lsx_vslt_w;
-   break;
-
- case E_V8HImode:
-   if (high_p != 0)
-     unpack = gen_lsx_vilvh_h;
-   else
-     unpack = gen_lsx_vilvl_h;
-
-   cmpFunc = gen_lsx_vslt_h;
-   break;
-
- case E_V16QImode:
-   if (high_p != 0)
-     unpack = gen_lsx_vilvh_b;
-   else
-     unpack = gen_lsx_vilvl_b;
-
-   cmpFunc = gen_lsx_vslt_b;
-   break;
-
- default:
-   gcc_unreachable ();
-   break;
- }
+	{
+	case E_V4SImode:
+	  if (high_p != 0)
+	    unpack = gen_lsx_vilvh_w;
+	  else
+	    unpack = gen_lsx_vilvl_w;
+
+	  cmpFunc = gen_lsx_vslt_w;
+	  break;
+
+	case E_V8HImode:
+	  if (high_p != 0)
+	    unpack = gen_lsx_vilvh_h;
+	  else
+	    unpack = gen_lsx_vilvl_h;
+
+	  cmpFunc = gen_lsx_vslt_h;
+	  break;
+
+	case E_V16QImode:
+	  if (high_p != 0)
+	    unpack = gen_lsx_vilvh_b;
+	  else
+	    unpack = gen_lsx_vilvl_b;
+
+	  cmpFunc = gen_lsx_vslt_b;
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	  break;
+	}
+
       if (!unsigned_p)
- {
-   /* Extract sign extention for each element comparing each element
-      with immediate zero.  */
-   tmp = gen_reg_rtx (imode);
-   emit_insn (cmpFunc (tmp, operands[1], CONST0_RTX (imode)));
- }
+	{
+	  /* Extract sign extention for each element comparing each element
+	     with immediate zero.  */
+	  tmp = gen_reg_rtx (imode);
+	  emit_insn (cmpFunc (tmp, operands[1], CONST0_RTX (imode)));
+	}
       else
- tmp = force_reg (imode, CONST0_RTX (imode));
+	tmp = force_reg (imode, CONST0_RTX (imode));
 
       dest = gen_reg_rtx (imode);
 
@@ -10180,433 +10192,402 @@ loongarch_expand_vector_group_init (rtx target, rtx vals)
 						      ops[1])));
 }
 
+/* Expand initialization of a vector which has all same elements.  */
+
+void
+loongarch_expand_vector_init_same (rtx target, rtx vals, unsigned nvar)
+{
+  machine_mode vmode = GET_MODE (target);
+  machine_mode imode = GET_MODE_INNER (vmode);
+  rtx same = XVECEXP (vals, 0, 0);
+  rtx temp, temp2;
+
+  if (CONST_INT_P (same) && nvar == 0
+      && loongarch_signed_immediate_p (INTVAL (same), 10, 0))
+    {
+      switch (vmode)
+	{
+	case E_V32QImode:
+	case E_V16HImode:
+	case E_V8SImode:
+	case E_V4DImode:
+	case E_V16QImode:
+	case E_V8HImode:
+	case E_V4SImode:
+	case E_V2DImode:
+	  temp = gen_rtx_CONST_VECTOR (vmode, XVEC (vals, 0));
+	  emit_move_insn (target, temp);
+	  return;
+	default:
+	  gcc_unreachable ();
+	}
+    }
+  temp = gen_reg_rtx (imode);
+  if (imode == GET_MODE (same))
+    temp2 = same;
+  else if (GET_MODE_SIZE (imode) >= UNITS_PER_WORD)
+    {
+      if (GET_CODE (same) == MEM)
+	{
+	  rtx reg_tmp = gen_reg_rtx (GET_MODE (same));
+	  loongarch_emit_move (reg_tmp, same);
+	  temp2 = simplify_gen_subreg (imode, reg_tmp, GET_MODE (reg_tmp), 0);
+	}
+      else
+	temp2 = simplify_gen_subreg (imode, same, GET_MODE (same), 0);
+    }
+  else
+    {
+      if (GET_CODE (same) == MEM)
+	{
+	  rtx reg_tmp = gen_reg_rtx (GET_MODE (same));
+	  loongarch_emit_move (reg_tmp, same);
+	  temp2 = lowpart_subreg (imode, reg_tmp, GET_MODE (reg_tmp));
+	}
+      else
+	temp2 = lowpart_subreg (imode, same, GET_MODE (same));
+    }
+  emit_move_insn (temp, temp2);
+
+  switch (vmode)
+    {
+    case E_V32QImode:
+    case E_V16HImode:
+    case E_V8SImode:
+    case E_V4DImode:
+    case E_V16QImode:
+    case E_V8HImode:
+    case E_V4SImode:
+    case E_V2DImode:
+      loongarch_emit_move (target, gen_rtx_VEC_DUPLICATE (vmode, temp));
+      break;
+
+    case E_V8SFmode:
+      emit_insn (gen_lasx_xvreplve0_w_f_scalar (target, temp));
+      break;
+
+    case E_V4DFmode:
+      emit_insn (gen_lasx_xvreplve0_d_f_scalar (target, temp));
+      break;
+
+    case E_V4SFmode:
+      emit_insn (gen_lsx_vreplvei_w_f_scalar (target, temp));
+      break;
+
+    case E_V2DFmode:
+      emit_insn (gen_lsx_vreplvei_d_f_scalar (target, temp));
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Expand a vector initialization.  */
+
 void
 loongarch_expand_vector_init (rtx target, rtx vals)
 {
   machine_mode vmode = GET_MODE (target);
   machine_mode imode = GET_MODE_INNER (vmode);
   unsigned i, nelt = GET_MODE_NUNITS (vmode);
-  unsigned nvar = 0;
-  bool all_same = true;
-  rtx x;
+  /* VALS is divided into high and low half-part.  */
+  /* Number of non constant elements in corresponding parts of VALS.  */
+  unsigned nvar = 0, hi_nvar = 0, lo_nvar = 0;
+  /* all_same : true if all elements of VALS are the same.
+     hi_same : true if all elements of the high half-part are the same.
+     lo_same : true if all elements of the low half-part are the same.
+     half_same : true if the high half-part is the same as the low one.  */
+  bool all_same = false, hi_same = true, lo_same = true, half_same = true;
+  rtx val[32], val_hi[32], val_lo[16];
+  rtx x, op0, op1;
+  /* Copy one element of vals to per element of target vector.  */
+  typedef rtx (*loongarch_vec_repl1_fn) (rtx, rtx);
+  /* Copy two elements of vals to target vector.  */
+  typedef rtx (*loongarch_vec_repl2_fn) (rtx, rtx, rtx);
+  /* Insert scalar operands into the specified position of the vector.  */
+  typedef rtx (*loongarch_vec_set_fn) (rtx, rtx, rtx);
+  /* Copy 64bit lowpart to highpart.  */
+  typedef rtx (*loongarch_vec_mirror_fn) (rtx, rtx, rtx);
+  /* Merge lowpart and highpart into target.  */
+  typedef rtx (*loongarch_vec_merge_fn) (rtx, rtx, rtx, rtx);
+
+  loongarch_vec_repl1_fn loongarch_vec_repl1_128 = NULL,
+			 loongarch_vec_repl1_256 = NULL;
+  loongarch_vec_repl2_fn loongarch_vec_repl2_128 = NULL,
+			 loongarch_vec_repl2_256 = NULL;
+  loongarch_vec_set_fn loongarch_vec_set128 = NULL, loongarch_vec_set256 = NULL;
+  loongarch_vec_mirror_fn loongarch_vec_mirror = NULL;
+  loongarch_vec_merge_fn loongarch_lasx_vecinit_merge = NULL;
+  machine_mode half_mode = VOIDmode;
+
+  /* Check whether elements of each part are the same.  */
+  for (i = 0; i < nelt / 2; ++i)
+    {
+      val_hi[i] = val_hi[i + nelt / 2] = val[i + nelt / 2]
+	= XVECEXP (vals, 0, i + nelt / 2);
+      val_lo[i] = val[i] = XVECEXP (vals, 0, i);
+      if (!loongarch_constant_elt_p (val_hi[i]))
+	hi_nvar++;
+      if (!loongarch_constant_elt_p (val_lo[i]))
+	lo_nvar++;
+      if (i > 0 && !rtx_equal_p (val_hi[i], val_hi[0]))
+	hi_same = false;
+      if (i > 0 && !rtx_equal_p (val_lo[i], val_lo[0]))
+	lo_same = false;
+      if (!rtx_equal_p (val_hi[i], val_lo[i]))
+	half_same = false;
+    }
+
+  /* If all elements are the same, set all_same true.  */
+  if (hi_same && lo_same && half_same)
+    all_same = true;
+
+  nvar = hi_nvar + lo_nvar;
 
-  for (i = 0; i < nelt; ++i)
+  switch (vmode)
     {
-      x = XVECEXP (vals, 0, i);
-      if (!loongarch_constant_elt_p (x))
- nvar++;
-      if (i > 0 && !rtx_equal_p (x, XVECEXP (vals, 0, 0)))
- all_same = false;
+    case E_V32QImode:
+      half_mode = E_V16QImode;
+      loongarch_vec_set256 = gen_vec_setv32qi_internal;
+      loongarch_vec_repl1_256 = gen_lasx_xvreplgr2vr_b;
+      loongarch_lasx_vecinit_merge
+	= half_same ? gen_lasx_xvpermi_q_v32qi : gen_lasx_vecinit_merge_v32qi;
+      /* FALLTHRU.  */
+    case E_V16QImode:
+      loongarch_vec_set128 = gen_vec_setv16qi;
+      loongarch_vec_repl1_128 = gen_lsx_vreplgr2vr_b;
+      loongarch_vec_mirror = gen_lsx_vreplvei_mirror_b;
+      break;
+
+    case E_V16HImode:
+      half_mode = E_V8HImode;
+      loongarch_vec_set256 = gen_vec_setv16hi_internal;
+      loongarch_vec_repl1_256 = gen_lasx_xvreplgr2vr_h;
+      loongarch_lasx_vecinit_merge
+	= half_same ? gen_lasx_xvpermi_q_v16hi : gen_lasx_vecinit_merge_v16hi;
+      /* FALLTHRU.  */
+    case E_V8HImode:
+      loongarch_vec_set128 = gen_vec_setv8hi;
+      loongarch_vec_repl1_128 = gen_lsx_vreplgr2vr_h;
+      loongarch_vec_mirror = gen_lsx_vreplvei_mirror_h;
+      break;
+
+    case E_V8SImode:
+      half_mode = V4SImode;
+      loongarch_vec_set256 = gen_vec_setv8si;
+      loongarch_vec_repl1_256 = gen_lasx_xvreplgr2vr_w;
+      loongarch_lasx_vecinit_merge
+	= half_same ? gen_lasx_xvpermi_q_v8si : gen_lasx_vecinit_merge_v8si;
+      /* FALLTHRU.  */
+    case E_V4SImode:
+      loongarch_vec_set128 = gen_vec_setv4si;
+      loongarch_vec_repl1_128 = gen_lsx_vreplgr2vr_w;
+      loongarch_vec_mirror = gen_lsx_vreplvei_mirror_w;
+      break;
+
+    case E_V4DImode:
+      half_mode = E_V2DImode;
+      loongarch_vec_set256 = gen_vec_setv4di;
+      loongarch_vec_repl1_256 = gen_lasx_xvreplgr2vr_d;
+      loongarch_lasx_vecinit_merge
+	= half_same ? gen_lasx_xvpermi_q_v4di : gen_lasx_vecinit_merge_v4di;
+      /* FALLTHRU.  */
+    case E_V2DImode:
+      loongarch_vec_set128 = gen_vec_setv2di;
+      loongarch_vec_repl1_128 = gen_lsx_vreplgr2vr_d;
+      loongarch_vec_mirror = gen_lsx_vreplvei_mirror_d;
+      break;
+
+    case E_V8SFmode:
+      half_mode = E_V4SFmode;
+      loongarch_vec_set256 = gen_vec_setv8sf;
+      loongarch_vec_repl1_128 = gen_lsx_vreplvei_w_f_scalar;
+      loongarch_vec_repl2_256 = gen_lasx_xvilvl_w_f_internal;
+      loongarch_lasx_vecinit_merge
+	= half_same ? gen_lasx_xvpermi_q_v8sf : gen_lasx_vecinit_merge_v8sf;
+      /* FALLTHRU.  */
+    case E_V4SFmode:
+      loongarch_vec_set128 = gen_vec_setv4sf;
+      loongarch_vec_repl2_128 = gen_lsx_vilvl_w_f_internal;
+      loongarch_vec_mirror = gen_lsx_vreplvei_mirror_w_f;
+      break;
+
+    case E_V4DFmode:
+      half_mode = E_V2DFmode;
+      loongarch_vec_set256 = gen_vec_setv4df;
+      loongarch_vec_repl1_128 = gen_lsx_vreplvei_d_f_scalar;
+      loongarch_vec_repl2_256 = gen_lasx_xvilvl_d_f_internal;
+      loongarch_lasx_vecinit_merge
+	= half_same ? gen_lasx_xvpermi_q_v4df : gen_lasx_vecinit_merge_v4df;
+      /* FALLTHRU.  */
+    case E_V2DFmode:
+      loongarch_vec_set128 = gen_vec_setv2df;
+      loongarch_vec_repl2_128 = gen_lsx_vilvl_d_f_internal;
+      loongarch_vec_mirror = gen_lsx_vreplvei_mirror_d_f;
+      break;
+
+    default:
+      gcc_unreachable ();
     }
+
   if (ISA_HAS_LASX && GET_MODE_SIZE (vmode) == 32)
     {
+      /* If all elements are the same, just do a broadcost.  */
       if (all_same)
-       {
-         rtx same = XVECEXP (vals, 0, 0);
-         rtx temp, temp2;
-
-         if (CONST_INT_P (same) && nvar == 0
-             && loongarch_signed_immediate_p (INTVAL (same), 10, 0))
-           {
-             switch (vmode)
-               {
-               case E_V32QImode:
-               case E_V16HImode:
-               case E_V8SImode:
-               case E_V4DImode:
-                 temp = gen_rtx_CONST_VECTOR (vmode, XVEC (vals, 0));
-                 emit_move_insn (target, temp);
-                 return;
-
-               default:
-                 gcc_unreachable ();
-               }
-           }
-
-         temp = gen_reg_rtx (imode);
-         if (imode == GET_MODE (same))
-           temp2 = same;
-         else if (GET_MODE_SIZE (imode) >= UNITS_PER_WORD)
-           {
-             if (GET_CODE (same) == MEM)
-               {
-                 rtx reg_tmp = gen_reg_rtx (GET_MODE (same));
-                 loongarch_emit_move (reg_tmp, same);
-                 temp2 = simplify_gen_subreg (imode, reg_tmp,
-                                              GET_MODE (reg_tmp), 0);
-               }
-             else
-               temp2 = simplify_gen_subreg (imode, same,
-                                            GET_MODE (same), 0);
-           }
-         else
-           {
-             if (GET_CODE (same) == MEM)
-               {
-                 rtx reg_tmp = gen_reg_rtx (GET_MODE (same));
-                 loongarch_emit_move (reg_tmp, same);
-                 temp2 = lowpart_subreg (imode, reg_tmp,
-                                         GET_MODE (reg_tmp));
-               }
-             else
-               temp2 = lowpart_subreg (imode, same, GET_MODE (same));
-           }
-         emit_move_insn (temp, temp2);
-
-         switch (vmode)
-           {
-           case E_V32QImode:
-           case E_V16HImode:
-           case E_V8SImode:
-           case E_V4DImode:
-             loongarch_emit_move (target,
-                                  gen_rtx_VEC_DUPLICATE (vmode, temp));
-             break;
-
-           case E_V8SFmode:
-             emit_insn (gen_lasx_xvreplve0_w_f_scalar (target, temp));
-             break;
-
-           case E_V4DFmode:
-             emit_insn (gen_lasx_xvreplve0_d_f_scalar (target, temp));
-             break;
-
-           default:
-             gcc_unreachable ();
-           }
-       }
+	loongarch_expand_vector_init_same (target, vals, nvar);
       else
-       {
-         rtvec vec = shallow_copy_rtvec (XVEC (vals, 0));
-
-         for (i = 0; i < nelt; ++i)
-           RTVEC_ELT (vec, i) = CONST0_RTX (imode);
-
-         emit_move_insn (target, gen_rtx_CONST_VECTOR (vmode, vec));
-
-         machine_mode half_mode = VOIDmode;
-         rtx target_hi, target_lo;
-
-         switch (vmode)
-           {
-           case E_V32QImode:
-             half_mode=E_V16QImode;
-             target_hi = gen_reg_rtx (half_mode);
-             target_lo = gen_reg_rtx (half_mode);
-             for (i = 0; i < nelt/2; ++i)
-               {
-                 rtx temp_hi = gen_reg_rtx (imode);
-                 rtx temp_lo = gen_reg_rtx (imode);
-                 emit_move_insn (temp_hi, XVECEXP (vals, 0, i+nelt/2));
-                 emit_move_insn (temp_lo, XVECEXP (vals, 0, i));
-                 if (i == 0)
-                   {
-                     emit_insn (gen_lsx_vreplvei_b_scalar (target_hi,
-                                                           temp_hi));
-                     emit_insn (gen_lsx_vreplvei_b_scalar (target_lo,
-                                                           temp_lo));
-                   }
-                 else
-                   {
-                     emit_insn (gen_vec_setv16qi (target_hi, temp_hi,
-                                                  GEN_INT (i)));
-                     emit_insn (gen_vec_setv16qi (target_lo, temp_lo,
-                                                  GEN_INT (i)));
-                   }
-               }
-             emit_insn (gen_rtx_SET (target,
-                                     gen_rtx_VEC_CONCAT (vmode, target_hi,
-                                                         target_lo)));
-             break;
-
-           case E_V16HImode:
-             half_mode=E_V8HImode;
-             target_hi = gen_reg_rtx (half_mode);
-             target_lo = gen_reg_rtx (half_mode);
-             for (i = 0; i < nelt/2; ++i)
-               {
-                 rtx temp_hi = gen_reg_rtx (imode);
-                 rtx temp_lo = gen_reg_rtx (imode);
-                 emit_move_insn (temp_hi, XVECEXP (vals, 0, i+nelt/2));
-                 emit_move_insn (temp_lo, XVECEXP (vals, 0, i));
-                 if (i == 0)
-                   {
-                     emit_insn (gen_lsx_vreplvei_h_scalar (target_hi,
-                                                           temp_hi));
-                     emit_insn (gen_lsx_vreplvei_h_scalar (target_lo,
-                                                           temp_lo));
-                   }
-                 else
-                   {
-                     emit_insn (gen_vec_setv8hi (target_hi, temp_hi,
-                                                 GEN_INT (i)));
-                     emit_insn (gen_vec_setv8hi (target_lo, temp_lo,
-                                                 GEN_INT (i)));
-                   }
-               }
-             emit_insn (gen_rtx_SET (target,
-                                     gen_rtx_VEC_CONCAT (vmode, target_hi,
-                                                         target_lo)));
-             break;
-
-           case E_V8SImode:
-             half_mode=V4SImode;
-             target_hi = gen_reg_rtx (half_mode);
-             target_lo = gen_reg_rtx (half_mode);
-             for (i = 0; i < nelt/2; ++i)
-               {
-                 rtx temp_hi = gen_reg_rtx (imode);
-                 rtx temp_lo = gen_reg_rtx (imode);
-                 emit_move_insn (temp_hi, XVECEXP (vals, 0, i+nelt/2));
-                 emit_move_insn (temp_lo, XVECEXP (vals, 0, i));
-                 if (i == 0)
-                   {
-                     emit_insn (gen_lsx_vreplvei_w_scalar (target_hi,
-                                                           temp_hi));
-                     emit_insn (gen_lsx_vreplvei_w_scalar (target_lo,
-                                                           temp_lo));
-                   }
-                 else
-                   {
-                     emit_insn (gen_vec_setv4si (target_hi, temp_hi,
-                                                 GEN_INT (i)));
-                     emit_insn (gen_vec_setv4si (target_lo, temp_lo,
-                                                 GEN_INT (i)));
-                   }
-               }
-             emit_insn (gen_rtx_SET (target,
-                                     gen_rtx_VEC_CONCAT (vmode, target_hi,
-                                                         target_lo)));
-             break;
-
-           case E_V4DImode:
-             half_mode=E_V2DImode;
-             target_hi = gen_reg_rtx (half_mode);
-             target_lo = gen_reg_rtx (half_mode);
-             for (i = 0; i < nelt/2; ++i)
-               {
-                 rtx temp_hi = gen_reg_rtx (imode);
-                 rtx temp_lo = gen_reg_rtx (imode);
-                 emit_move_insn (temp_hi, XVECEXP (vals, 0, i+nelt/2));
-                 emit_move_insn (temp_lo, XVECEXP (vals, 0, i));
-                 if (i == 0)
-                   {
-                     emit_insn (gen_lsx_vreplvei_d_scalar (target_hi,
-                                                           temp_hi));
-                     emit_insn (gen_lsx_vreplvei_d_scalar (target_lo,
-                                                           temp_lo));
-                   }
-                 else
-                   {
-                     emit_insn (gen_vec_setv2di (target_hi, temp_hi,
-                                                 GEN_INT (i)));
-                     emit_insn (gen_vec_setv2di (target_lo, temp_lo,
-                                                 GEN_INT (i)));
-                   }
-               }
-             emit_insn (gen_rtx_SET (target,
-                                     gen_rtx_VEC_CONCAT (vmode, target_hi,
-                                                         target_lo)));
-             break;
-
-           case E_V8SFmode:
-             half_mode=E_V4SFmode;
-             target_hi = gen_reg_rtx (half_mode);
-             target_lo = gen_reg_rtx (half_mode);
-             for (i = 0; i < nelt/2; ++i)
-               {
-                 rtx temp_hi = gen_reg_rtx (imode);
-                 rtx temp_lo = gen_reg_rtx (imode);
-                 emit_move_insn (temp_hi, XVECEXP (vals, 0, i+nelt/2));
-                 emit_move_insn (temp_lo, XVECEXP (vals, 0, i));
-                 if (i == 0)
-                   {
-                     emit_insn (gen_lsx_vreplvei_w_f_scalar (target_hi,
-                                                             temp_hi));
-                     emit_insn (gen_lsx_vreplvei_w_f_scalar (target_lo,
-                                                             temp_lo));
-                   }
-                 else
-                   {
-                     emit_insn (gen_vec_setv4sf (target_hi, temp_hi,
-                                                 GEN_INT (i)));
-                     emit_insn (gen_vec_setv4sf (target_lo, temp_lo,
-                                                 GEN_INT (i)));
-                   }
-               }
-             emit_insn (gen_rtx_SET (target,
-                                     gen_rtx_VEC_CONCAT (vmode, target_hi,
-                                                         target_lo)));
-             break;
-
-           case E_V4DFmode:
-             half_mode=E_V2DFmode;
-             target_hi = gen_reg_rtx (half_mode);
-             target_lo = gen_reg_rtx (half_mode);
-             for (i = 0; i < nelt/2; ++i)
-               {
-                 rtx temp_hi = gen_reg_rtx (imode);
-                 rtx temp_lo = gen_reg_rtx (imode);
-                 emit_move_insn (temp_hi, XVECEXP (vals, 0, i+nelt/2));
-                 emit_move_insn (temp_lo, XVECEXP (vals, 0, i));
-                 if (i == 0)
-                   {
-                     emit_insn (gen_lsx_vreplvei_d_f_scalar (target_hi,
-                                                             temp_hi));
-                     emit_insn (gen_lsx_vreplvei_d_f_scalar (target_lo,
-                                                             temp_lo));
-                   }
-                 else
-                   {
-                     emit_insn (gen_vec_setv2df (target_hi, temp_hi,
-                                                 GEN_INT (i)));
-                     emit_insn (gen_vec_setv2df (target_lo, temp_lo,
-                                                 GEN_INT (i)));
-                   }
-               }
-             emit_insn (gen_rtx_SET (target,
-                                     gen_rtx_VEC_CONCAT (vmode, target_hi,
-                                                         target_lo)));
-             break;
-
-           default:
-             gcc_unreachable ();
-           }
-
-       }
+	{
+	  gcc_assert (nelt >= 4);
+
+	  rtx target_hi, target_lo;
+	  /* Write elements of high half-part in target directly.  */
+	  target_hi = target;
+	  target_lo = gen_reg_rtx (half_mode);
+
+	  /* If all elements of high half-part are the same,
+	     just do a broadcost.  Also applicable to low half-part.  */
+	  if (hi_same)
+	    {
+	      rtx vtmp = gen_rtx_PARALLEL (vmode, gen_rtvec_v (nelt, val_hi));
+	      loongarch_expand_vector_init_same (target_hi, vtmp, hi_nvar);
+	    }
+	  if (lo_same)
+	    {
+	      rtx vtmp
+		= gen_rtx_PARALLEL (half_mode, gen_rtvec_v (nelt / 2, val_lo));
+	      loongarch_expand_vector_init_same (target_lo, vtmp, lo_nvar);
+	    }
+
+	  for (i = 0; i < nelt / 2; ++i)
+	    {
+	      if (!hi_same)
+		{
+		  if (vmode == E_V8SFmode || vmode == E_V4DFmode)
+		    {
+		      /* Using xvilvl to load lowest 2 elements simultaneously
+			 to reduce the number of instructions.  */
+		      if (i == 1)
+			{
+			  op0 = gen_reg_rtx (imode);
+			  emit_move_insn (op0, val_hi[0]);
+			  op1 = gen_reg_rtx (imode);
+			  emit_move_insn (op1, val_hi[1]);
+			  emit_insn (
+			    loongarch_vec_repl2_256 (target_hi, op0, op1));
+			}
+		      else if (i > 1)
+			{
+			  op0 = gen_reg_rtx (imode);
+			  emit_move_insn (op0, val_hi[i]);
+			  emit_insn (
+			    loongarch_vec_set256 (target_hi, op0, GEN_INT (i)));
+			}
+		    }
+		  else
+		    {
+		      /* Assign the lowest element of val_hi to all elements
+			 of target_hi.  */
+		      if (i == 0)
+			{
+			  op0 = gen_reg_rtx (imode);
+			  emit_move_insn (op0, val_hi[0]);
+			  emit_insn (loongarch_vec_repl1_256 (target_hi, op0));
+			}
+		      else if (!rtx_equal_p (val_hi[i], val_hi[0]))
+			{
+			  op0 = gen_reg_rtx (imode);
+			  emit_move_insn (op0, val_hi[i]);
+			  emit_insn (
+			    loongarch_vec_set256 (target_hi, op0, GEN_INT (i)));
+			}
+		    }
+		}
+	      if (!lo_same && !half_same)
+		{
+		  /* Assign the lowest element of val_lo to all elements
+		     of target_lo.  */
+		  if (i == 0)
+		    {
+		      op0 = gen_reg_rtx (imode);
+		      emit_move_insn (op0, val_lo[0]);
+		      emit_insn (loongarch_vec_repl1_128 (target_lo, op0));
+		    }
+		  else if (!rtx_equal_p (val_lo[i], val_lo[0]))
+		    {
+		      op0 = gen_reg_rtx (imode);
+		      emit_move_insn (op0, val_lo[i]);
+		      emit_insn (
+			loongarch_vec_set128 (target_lo, op0, GEN_INT (i)));
+		    }
+		}
+	    }
+	  if (half_same)
+	    {
+	      emit_insn (loongarch_lasx_vecinit_merge (target, target_hi,
+						       target_hi, const0_rtx));
+	      return;
+	    }
+	  emit_insn (loongarch_lasx_vecinit_merge (target, target_hi, target_lo,
+						   GEN_INT (0x20)));
+	}
       return;
     }
 
   if (ISA_HAS_LSX)
     {
       if (all_same)
- {
-   rtx same = XVECEXP (vals, 0, 0);
-   rtx temp, temp2;
-
-   if (CONST_INT_P (same) && nvar == 0
-       && loongarch_signed_immediate_p (INTVAL (same), 10, 0))
-     {
-       switch (vmode)
-   {
-   case E_V16QImode:
-   case E_V8HImode:
-   case E_V4SImode:
-   case E_V2DImode:
-     temp = gen_rtx_CONST_VECTOR (vmode, XVEC (vals, 0));
-     emit_move_insn (target, temp);
-     return;
-
-   default:
-     gcc_unreachable ();
-   }
-     }
-   temp = gen_reg_rtx (imode);
-   if (imode == GET_MODE (same))
-     temp2 = same;
-   else if (GET_MODE_SIZE (imode) >= UNITS_PER_WORD)
-     {
-       if (GET_CODE (same) == MEM)
-   {
-     rtx reg_tmp = gen_reg_rtx (GET_MODE (same));
-     loongarch_emit_move (reg_tmp, same);
-     temp2 = simplify_gen_subreg (imode, reg_tmp,
-                GET_MODE (reg_tmp), 0);
-   }
-       else
-   temp2 = simplify_gen_subreg (imode, same, GET_MODE (same), 0);
-     }
-   else
-     {
-       if (GET_CODE (same) == MEM)
-   {
-     rtx reg_tmp = gen_reg_rtx (GET_MODE (same));
-     loongarch_emit_move (reg_tmp, same);
-     temp2 = lowpart_subreg (imode, reg_tmp, GET_MODE (reg_tmp));
-   }
-       else
-   temp2 = lowpart_subreg (imode, same, GET_MODE (same));
-     }
-   emit_move_insn (temp, temp2);
-
-   switch (vmode)
-     {
-     case E_V16QImode:
-     case E_V8HImode:
-     case E_V4SImode:
-     case E_V2DImode:
-       loongarch_emit_move (target, gen_rtx_VEC_DUPLICATE (vmode, temp));
-       break;
-
-     case E_V4SFmode:
-       emit_insn (gen_lsx_vreplvei_w_f_scalar (target, temp));
-       break;
-
-     case E_V2DFmode:
-       emit_insn (gen_lsx_vreplvei_d_f_scalar (target, temp));
-       break;
-
-     default:
-       gcc_unreachable ();
-     }
- }
+	loongarch_expand_vector_init_same (target, vals, nvar);
       else
- {
-   emit_move_insn (target, CONST0_RTX (vmode));
-
-   for (i = 0; i < nelt; ++i)
-     {
-       rtx temp = gen_reg_rtx (imode);
-       emit_move_insn (temp, XVECEXP (vals, 0, i));
-       switch (vmode)
-   {
-   case E_V16QImode:
-     if (i == 0)
-       emit_insn (gen_lsx_vreplvei_b_scalar (target, temp));
-     else
-       emit_insn (gen_vec_setv16qi (target, temp, GEN_INT (i)));
-     break;
-
-   case E_V8HImode:
-     if (i == 0)
-       emit_insn (gen_lsx_vreplvei_h_scalar (target, temp));
-     else
-       emit_insn (gen_vec_setv8hi (target, temp, GEN_INT (i)));
-     break;
-
-   case E_V4SImode:
-     if (i == 0)
-       emit_insn (gen_lsx_vreplvei_w_scalar (target, temp));
-     else
-       emit_insn (gen_vec_setv4si (target, temp, GEN_INT (i)));
-     break;
-
-   case E_V2DImode:
-     if (i == 0)
-       emit_insn (gen_lsx_vreplvei_d_scalar (target, temp));
-     else
-       emit_insn (gen_vec_setv2di (target, temp, GEN_INT (i)));
-     break;
-
-   case E_V4SFmode:
-     if (i == 0)
-       emit_insn (gen_lsx_vreplvei_w_f_scalar (target, temp));
-     else
-       emit_insn (gen_vec_setv4sf (target, temp, GEN_INT (i)));
-     break;
-
-   case E_V2DFmode:
-     if (i == 0)
-       emit_insn (gen_lsx_vreplvei_d_f_scalar (target, temp));
-     else
-       emit_insn (gen_vec_setv2df (target, temp, GEN_INT (i)));
-     break;
-
-   default:
-     gcc_unreachable ();
-   }
-     }
- }
+	{
+	  for (i = 0; i < nelt; ++i)
+	    {
+	      if (vmode == E_V4SFmode || vmode == E_V2DFmode)
+		{
+		  /* Using vilvl to load lowest 2 elements simultaneously to
+		     reduce the number of instructions.  */
+		  if (i == 1)
+		    {
+		      op0 = gen_reg_rtx (imode);
+		      emit_move_insn (op0, val[0]);
+		      op1 = gen_reg_rtx (imode);
+		      emit_move_insn (op1, val[1]);
+		      emit_insn (loongarch_vec_repl2_128 (target, op0, op1));
+		    }
+		  else if (i > 1)
+		    {
+		      op0 = gen_reg_rtx (imode);
+		      emit_move_insn (op0, val[i]);
+		      emit_insn (
+			loongarch_vec_set128 (target, op0, GEN_INT (i)));
+		    }
+		}
+	      else
+		{
+		  if (half_same && i == nelt / 2)
+		    {
+		      emit_insn (
+			loongarch_vec_mirror (target, target, const0_rtx));
+		      return;
+		    }
+		  /* Assign the lowest element of val to all elements of
+		     target.  */
+		  if (i == 0)
+		    {
+		      op0 = gen_reg_rtx (imode);
+		      emit_move_insn (op0, val[0]);
+		      emit_insn (loongarch_vec_repl1_128 (target, op0));
+		    }
+		  else if (!rtx_equal_p (val[i], val[0]))
+		    {
+		      op0 = gen_reg_rtx (imode);
+		      emit_move_insn (op0, val[i]);
+		      emit_insn (
+			loongarch_vec_set128 (target, op0, GEN_INT (i)));
+		    }
+		}
+	    }
+	}
       return;
     }
 
@@ -10620,8 +10601,8 @@ loongarch_expand_vector_init (rtx target, rtx vals)
   /* For two-part initialization, always use CONCAT.  */
   if (nelt == 2)
     {
-      rtx op0 = force_reg (imode, XVECEXP (vals, 0, 0));
-      rtx op1 = force_reg (imode, XVECEXP (vals, 0, 1));
+      rtx op0 = force_reg (imode, val[0]);
+      rtx op1 = force_reg (imode, val[1]);
       x = gen_rtx_VEC_CONCAT (vmode, op0, op1);
       emit_insn (gen_rtx_SET (target, x));
       return;
@@ -10635,7 +10616,7 @@ loongarch_expand_vector_init (rtx target, rtx vals)
 
 machine_mode
 loongarch_hard_regno_caller_save_mode (unsigned int regno, unsigned int nregs,
-              machine_mode mode)
+				       machine_mode mode)
 {
   /* For performance, avoid saving/restoring upper parts of a register
      by returning MODE as save mode when the mode is known.  */
@@ -10666,96 +10647,96 @@ loongarch_expand_lsx_cmp (rtx dest, enum rtx_code cond, rtx op0, rtx op1)
     case E_V2DImode:
     case E_V4DImode:
       switch (cond)
- {
- case NE:
-   cond = reverse_condition (cond);
-   negate = true;
-   break;
- case EQ:
- case LT:
- case LE:
- case LTU:
- case LEU:
-   break;
- case GE:
- case GT:
- case GEU:
- case GTU:
-   std::swap (op0, op1);
-   cond = swap_condition (cond);
-   break;
- default:
-   gcc_unreachable ();
- }
+	{
+	case NE:
+	  cond = reverse_condition (cond);
+	  negate = true;
+	  break;
+	case EQ:
+	case LT:
+	case LE:
+	case LTU:
+	case LEU:
+	  break;
+	case GE:
+	case GT:
+	case GEU:
+	case GTU:
+	  std::swap (op0, op1);
+	  cond = swap_condition (cond);
+	  break;
+	default:
+	  gcc_unreachable ();
+	}
       loongarch_emit_binary (cond, dest, op0, op1);
       if (negate)
- emit_move_insn (dest, gen_rtx_NOT (GET_MODE (dest), dest));
+	emit_move_insn (dest, gen_rtx_NOT (GET_MODE (dest), dest));
       break;
 
     case E_V4SFmode:
     case E_V2DFmode:
       switch (cond)
- {
- case UNORDERED:
- case ORDERED:
- case EQ:
- case NE:
- case UNEQ:
- case UNLE:
- case UNLT:
-   break;
- case LTGT: cond = NE; break;
- case UNGE: cond = UNLE; std::swap (op0, op1); break;
- case UNGT: cond = UNLT; std::swap (op0, op1); break;
- case LE: unspec = UNSPEC_LSX_VFCMP_SLE; break;
- case LT: unspec = UNSPEC_LSX_VFCMP_SLT; break;
- case GE: unspec = UNSPEC_LSX_VFCMP_SLE; std::swap (op0, op1); break;
- case GT: unspec = UNSPEC_LSX_VFCMP_SLT; std::swap (op0, op1); break;
- default:
-    gcc_unreachable ();
- }
+	{
+	case UNORDERED:
+	case ORDERED:
+	case EQ:
+	case NE:
+	case UNEQ:
+	case UNLE:
+	case UNLT:
+	  break;
+	case LTGT: cond = NE; break;
+	case UNGE: cond = UNLE; std::swap (op0, op1); break;
+	case UNGT: cond = UNLT; std::swap (op0, op1); break;
+	case LE: unspec = UNSPEC_LSX_VFCMP_SLE; break;
+	case LT: unspec = UNSPEC_LSX_VFCMP_SLT; break;
+	case GE: unspec = UNSPEC_LSX_VFCMP_SLE; std::swap (op0, op1); break;
+	case GT: unspec = UNSPEC_LSX_VFCMP_SLT; std::swap (op0, op1); break;
+	default:
+		 gcc_unreachable ();
+	}
       if (unspec < 0)
- loongarch_emit_binary (cond, dest, op0, op1);
+	loongarch_emit_binary (cond, dest, op0, op1);
       else
- {
-   rtx x = gen_rtx_UNSPEC (GET_MODE (dest),
-         gen_rtvec (2, op0, op1), unspec);
-   emit_insn (gen_rtx_SET (dest, x));
- }
+	{
+	  rtx x = gen_rtx_UNSPEC (GET_MODE (dest),
+				  gen_rtvec (2, op0, op1), unspec);
+	  emit_insn (gen_rtx_SET (dest, x));
+	}
       break;
 
     case E_V8SFmode:
     case E_V4DFmode:
       switch (cond)
- {
- case UNORDERED:
- case ORDERED:
- case EQ:
- case NE:
- case UNEQ:
- case UNLE:
- case UNLT:
-   break;
- case LTGT: cond = NE; break;
- case UNGE: cond = UNLE; std::swap (op0, op1); break;
- case UNGT: cond = UNLT; std::swap (op0, op1); break;
- case LE: unspec = UNSPEC_LASX_XVFCMP_SLE; break;
- case LT: unspec = UNSPEC_LASX_XVFCMP_SLT; break;
- case GE: unspec = UNSPEC_LASX_XVFCMP_SLE; std::swap (op0, op1); break;
- case GT: unspec = UNSPEC_LASX_XVFCMP_SLT; std::swap (op0, op1); break;
- default:
-    gcc_unreachable ();
- }
+	{
+	case UNORDERED:
+	case ORDERED:
+	case EQ:
+	case NE:
+	case UNEQ:
+	case UNLE:
+	case UNLT:
+	  break;
+	case LTGT: cond = NE; break;
+	case UNGE: cond = UNLE; std::swap (op0, op1); break;
+	case UNGT: cond = UNLT; std::swap (op0, op1); break;
+	case LE: unspec = UNSPEC_LASX_XVFCMP_SLE; break;
+	case LT: unspec = UNSPEC_LASX_XVFCMP_SLT; break;
+	case GE: unspec = UNSPEC_LASX_XVFCMP_SLE; std::swap (op0, op1); break;
+	case GT: unspec = UNSPEC_LASX_XVFCMP_SLT; std::swap (op0, op1); break;
+	default:
+		 gcc_unreachable ();
+	}
       if (unspec < 0)
- loongarch_emit_binary (cond, dest, op0, op1);
+	loongarch_emit_binary (cond, dest, op0, op1);
       else
- {
-   rtx x = gen_rtx_UNSPEC (GET_MODE (dest),
-         gen_rtvec (2, op0, op1), unspec);
-   emit_insn (gen_rtx_SET (dest, x));
- }
+	{
+	  rtx x = gen_rtx_UNSPEC (GET_MODE (dest),
+				  gen_rtvec (2, op0, op1), unspec);
+	  emit_insn (gen_rtx_SET (dest, x));
+	}
       break;
-      
+
     default:
       gcc_unreachable ();
       break;
@@ -10769,7 +10750,7 @@ loongarch_expand_lsx_cmp (rtx dest, enum rtx_code cond, rtx op0, rtx op1)
 
 void
 loongarch_expand_vec_cond_expr (machine_mode mode, machine_mode vimode,
-       rtx *operands)
+				rtx *operands)
 {
   rtx cond = operands[3];
   rtx cmp_op0 = operands[4];
@@ -10799,45 +10780,45 @@ loongarch_expand_vec_cond_expr (machine_mode mode, machine_mode vimode,
       emit_move_insn (mask, cmp_res);
 
       if (register_operand (operands[1], mode))
- {
-   rtx xop1 = operands[1];
-   if (mode != vimode)
-     {
-       xop1 = gen_reg_rtx (vimode);
-       emit_move_insn (xop1, gen_rtx_SUBREG (vimode, operands[1], 0));
-     }
-   emit_move_insn (src1, xop1);
- }
+	{
+	  rtx xop1 = operands[1];
+	  if (mode != vimode)
+	    {
+	      xop1 = gen_reg_rtx (vimode);
+	      emit_move_insn (xop1, gen_rtx_SUBREG (vimode, operands[1], 0));
+	    }
+	  emit_move_insn (src1, xop1);
+	}
       else
- {
-   gcc_assert (operands[1] == CONSTM1_RTX (vimode));
-   /* Case (2) if the below doesn't move the mask to src2.  */
-   emit_move_insn (src1, mask);
- }
+	{
+	  gcc_assert (operands[1] == CONSTM1_RTX (vimode));
+	  /* Case (2) if the below doesn't move the mask to src2.  */
+	  emit_move_insn (src1, mask);
+	}
 
       if (register_operand (operands[2], mode))
- {
-   rtx xop2 = operands[2];
-   if (mode != vimode)
-     {
-       xop2 = gen_reg_rtx (vimode);
-       emit_move_insn (xop2, gen_rtx_SUBREG (vimode, operands[2], 0));
-     }
-   emit_move_insn (src2, xop2);
- }
+	{
+	  rtx xop2 = operands[2];
+	  if (mode != vimode)
+	    {
+	      xop2 = gen_reg_rtx (vimode);
+	      emit_move_insn (xop2, gen_rtx_SUBREG (vimode, operands[2], 0));
+	    }
+	  emit_move_insn (src2, xop2);
+	}
       else
- {
-   gcc_assert (operands[2] == CONST0_RTX (mode));
-   /* Case (3) if the above didn't move the mask to src1.  */
-   emit_move_insn (src2, mask);
- }
+	{
+	  gcc_assert (operands[2] == CONST0_RTX (mode));
+	  /* Case (3) if the above didn't move the mask to src1.  */
+	  emit_move_insn (src2, mask);
+	}
 
       /* We deal with case (4) if the mask wasn't moved to either src1 or src2.
-  In any case, we eventually do vector mask-based copy.  */
+	 In any case, we eventually do vector mask-based copy.  */
       bsel = gen_rtx_IOR (vimode,
-       gen_rtx_AND (vimode,
-              gen_rtx_NOT (vimode, mask), src2),
-       gen_rtx_AND (vimode, mask, src1));
+			  gen_rtx_AND (vimode,
+				       gen_rtx_NOT (vimode, mask), src2),
+			  gen_rtx_AND (vimode, mask, src1));
       /* The result is placed back to a register with the mask.  */
       emit_insn (gen_rtx_SET (mask, bsel));
       emit_move_insn (operands[0], gen_rtx_SUBREG (mode, mask, 0));
@@ -10846,7 +10827,7 @@ loongarch_expand_vec_cond_expr (machine_mode mode, machine_mode vimode,
 
 void
 loongarch_expand_vec_cond_mask_expr (machine_mode mode, machine_mode vimode,
-           rtx *operands)
+				    rtx *operands)
 {
   rtx cmp_res = operands[3];
 
@@ -10871,45 +10852,45 @@ loongarch_expand_vec_cond_mask_expr (machine_mode mode, machine_mode vimode,
       emit_move_insn (mask, cmp_res);
 
       if (register_operand (operands[1], mode))
- {
-   rtx xop1 = operands[1];
-   if (mode != vimode)
-     {
-       xop1 = gen_reg_rtx (vimode);
-       emit_move_insn (xop1, gen_rtx_SUBREG (vimode, operands[1], 0));
-     }
-   emit_move_insn (src1, xop1);
- }
+	{
+	  rtx xop1 = operands[1];
+	  if (mode != vimode)
+	    {
+	      xop1 = gen_reg_rtx (vimode);
+	      emit_move_insn (xop1, gen_rtx_SUBREG (vimode, operands[1], 0));
+	    }
+	  emit_move_insn (src1, xop1);
+	}
       else
- {
-   gcc_assert (operands[1] == CONSTM1_RTX (vimode));
-   /* Case (2) if the below doesn't move the mask to src2.  */
-   emit_move_insn (src1, mask);
- }
+	{
+	  gcc_assert (operands[1] == CONSTM1_RTX (vimode));
+	  /* Case (2) if the below doesn't move the mask to src2.  */
+	  emit_move_insn (src1, mask);
+	}
 
       if (register_operand (operands[2], mode))
- {
-   rtx xop2 = operands[2];
-   if (mode != vimode)
-     {
-       xop2 = gen_reg_rtx (vimode);
-       emit_move_insn (xop2, gen_rtx_SUBREG (vimode, operands[2], 0));
-     }
-   emit_move_insn (src2, xop2);
- }
+	{
+	  rtx xop2 = operands[2];
+	  if (mode != vimode)
+	    {
+	      xop2 = gen_reg_rtx (vimode);
+	      emit_move_insn (xop2, gen_rtx_SUBREG (vimode, operands[2], 0));
+	    }
+	  emit_move_insn (src2, xop2);
+	}
       else
- {
-   gcc_assert (operands[2] == CONST0_RTX (mode));
-   /* Case (3) if the above didn't move the mask to src1.  */
-   emit_move_insn (src2, mask);
- }
+	{
+	  gcc_assert (operands[2] == CONST0_RTX (mode));
+	  /* Case (3) if the above didn't move the mask to src1.  */
+	  emit_move_insn (src2, mask);
+	}
 
       /* We deal with case (4) if the mask wasn't moved to either src1 or src2.
-  In any case, we eventually do vector mask-based copy.  */
+	 In any case, we eventually do vector mask-based copy.  */
       bsel = gen_rtx_IOR (vimode,
-       gen_rtx_AND (vimode,
-              gen_rtx_NOT (vimode, mask), src2),
-       gen_rtx_AND (vimode, mask, src1));
+			  gen_rtx_AND (vimode,
+				       gen_rtx_NOT (vimode, mask), src2),
+			  gen_rtx_AND (vimode, mask, src1));
       /* The result is placed back to a register with the mask.  */
       emit_insn (gen_rtx_SET (mask, bsel));
       emit_move_insn (operands[0], gen_rtx_SUBREG (mode, mask, 0));
@@ -10938,7 +10919,7 @@ loongarch_case_values_threshold (void)
 
 static reg_class_t
 loongarch_spill_class (reg_class_t rclass ATTRIBUTE_UNUSED,
-          machine_mode mode ATTRIBUTE_UNUSED)
+		       machine_mode mode ATTRIBUTE_UNUSED)
 {
   return NO_REGS;
 }
@@ -10953,10 +10934,10 @@ loongarch_spill_class (reg_class_t rclass ATTRIBUTE_UNUSED,
 
 static machine_mode
 loongarch_promote_function_mode (const_tree type ATTRIBUTE_UNUSED,
-        machine_mode mode,
-        int *punsignedp ATTRIBUTE_UNUSED,
-        const_tree fntype ATTRIBUTE_UNUSED,
-        int for_return ATTRIBUTE_UNUSED)
+				 machine_mode mode,
+				 int *punsignedp ATTRIBUTE_UNUSED,
+				 const_tree fntype ATTRIBUTE_UNUSED,
+				 int for_return ATTRIBUTE_UNUSED)
 {
   int unsignedp;
 
@@ -11017,7 +10998,7 @@ loongarch_build_const_vector (machine_mode mode, bool vect, rtx value)
       RTVEC_ELT (v, 0) = value;
 
       for (i = 1; i < n_elt; ++i)
- RTVEC_ELT (v, i) = vect ? value : CONST0_RTX (scalar_mode);
+	RTVEC_ELT (v, i) = vect ? value : CONST0_RTX (scalar_mode);
 
       return gen_rtx_CONST_VECTOR (mode, v);
 
@@ -11025,6 +11006,7 @@ loongarch_build_const_vector (machine_mode mode, bool vect, rtx value)
       gcc_unreachable ();
     }
 }
+
 /* Create a mask for the sign bit in MODE
    for an register.  If VECT is true, then replicate the mask for
    all elements of the vector register.  If INVERT is true, then create
@@ -11071,7 +11053,7 @@ loongarch_build_signbit_mask (machine_mode mode, bool vect, bool invert)
 
   machine_mode inner_mode = GET_MODE_INNER (mode);
   w = wi::set_bit_in_zero (GET_MODE_BITSIZE (inner_mode) - 1,
-        GET_MODE_BITSIZE (inner_mode));
+			   GET_MODE_BITSIZE (inner_mode));
   if (invert)
     w = wi::bit_not (w);
 
@@ -11088,19 +11070,19 @@ loongarch_build_signbit_mask (machine_mode mode, bool vect, bool invert)
 
 static bool
 loongarch_builtin_support_vector_misalignment (machine_mode mode,
-                const_tree type,
-                int misalignment,
-                bool is_packed)
+					       const_tree type,
+					       int misalignment,
+					       bool is_packed)
 {
   if ((ISA_HAS_LSX || ISA_HAS_LASX) && STRICT_ALIGNMENT)
     {
       if (optab_handler (movmisalign_optab, mode) == CODE_FOR_nothing)
- return false;
+	return false;
       if (misalignment == -1)
- return false;
+	return false;
     }
   return default_builtin_support_vector_misalignment (mode, type, misalignment,
-                 is_packed);
+						      is_packed);
 }
 
 /* Initialize the GCC target structure.  */
@@ -11184,7 +11166,7 @@ loongarch_builtin_support_vector_misalignment (machine_mode mode,
 #define TARGET_ASM_CAN_OUTPUT_MI_THUNK \
   hook_bool_const_tree_hwi_hwi_const_tree_true
 
-3 +#undef TARGET_PRINT_OPERAND
+#undef TARGET_PRINT_OPERAND
 #define TARGET_PRINT_OPERAND loongarch_print_operand
 #undef TARGET_PRINT_OPERAND_ADDRESS
 #define TARGET_PRINT_OPERAND_ADDRESS loongarch_print_operand_address
diff --git a/src/gcc/config/loongarch/loongarch.h b/src/gcc/config/loongarch/loongarch.h
index efde5b8e7..c16d4b79a 100644
--- a/src/gcc/config/loongarch/loongarch.h
+++ b/src/gcc/config/loongarch/loongarch.h
@@ -1061,6 +1061,11 @@ typedef struct {
 
 #define ASM_OUTPUT_ALIGN(STREAM, LOG) fprintf (STREAM, "\t.align\t%d\n", (LOG))
 
+/* "nop" instruction 54525952 (andi $r0,$r0,0) is
+   used for padding.  */
+#define ASM_OUTPUT_ALIGN_WITH_NOP(STREAM, LOG) \
+  fprintf (STREAM, "\t.align\t%d,54525952,4\n", (LOG))
+
 /* This is how to output an assembler line to advance the location
    counter by SIZE bytes.  */
 
diff --git a/src/gcc/config/loongarch/loongarch.opt b/src/gcc/config/loongarch/loongarch.opt
index 26d381a97..78f2baf3a 100644
--- a/src/gcc/config/loongarch/loongarch.opt
+++ b/src/gcc/config/loongarch/loongarch.opt
@@ -90,7 +90,7 @@ Enum(isa_ext_simd) String(lasx) Value(ISA_EXT_SIMD_LASX)
 
 msimd=
 Target RejectNegative Joined ToLower Enum(isa_ext_simd) Var(la_opt_simd) Init(M_OPT_UNSET)
--msimd=SIMD  Generate code for the given SIMD extension.
+-msimd=SIMD	Generate code for the given SIMD extension.
 
 mlsx
 Target Driver Defer Var(la_deferred_options)
@@ -150,13 +150,13 @@ mabi=
 Target RejectNegative Joined ToLower Enum(abi_base) Var(la_opt_abi_base) Init(M_OPT_UNSET)
 -mabi=BASEABI	Generate code that conforms to the given BASEABI.
 
+
 ;; ABI Extension
 Variable
 int la_opt_abi_ext = M_OPT_UNSET
 
-
 mbranch-cost=
-Target RejectNegative Joined UInteger Var(loongarch_branch_cost) Save
+Target RejectNegative Joined UInteger Var(loongarch_branch_cost)
 -mbranch-cost=COST	Set the cost of branches to roughly COST instructions.
 
 mmemvec-cost=
@@ -164,32 +164,32 @@ Target RejectNegative Joined UInteger Var(loongarch_vector_access_cost) IntegerR
 mmemvec-cost=COST      Set the cost of vector memory access instructions.
 
 mcheck-zero-division
-Target Mask(CHECK_ZERO_DIV) Save
+Target Mask(CHECK_ZERO_DIV)
 Trap on integer divide by zero.
 
 mcond-move-int
-Target Var(TARGET_COND_MOVE_INT) Init(1) Save
+Target Var(TARGET_COND_MOVE_INT) Init(1)
 Conditional moves for integral are enabled.
 
 mcond-move-float
-Target Var(TARGET_COND_MOVE_FLOAT) Init(1) Save
+Target Var(TARGET_COND_MOVE_FLOAT) Init(1)
 Conditional moves for float are enabled.
 
 mmemcpy
-Target Mask(MEMCPY) Save
+Target Mask(MEMCPY)
 Prevent optimizing block moves, which is also the default behavior of -Os.
 
 mstrict-align
-Target Var(TARGET_STRICT_ALIGN) Init(0) Save
+Target Var(TARGET_STRICT_ALIGN) Init(0)
 Do not generate unaligned memory accesses.
 
 mmax-inline-memcpy-size=
-Target Joined RejectNegative UInteger Var(loongarch_max_inline_memcpy_size) Init(1024) Save
+Target Joined RejectNegative UInteger Var(loongarch_max_inline_memcpy_size) Init(1024)
 -mmax-inline-memcpy-size=SIZE	Set the max size of memcpy to inline, default is 1024.
 
 mexplicit-relocs
 Target Var(TARGET_EXPLICIT_RELOCS) Init(HAVE_AS_EXPLICIT_RELOCS & !HAVE_AS_MRELAX_OPTION)
-Use %reloc() assembly operators
+Use %reloc() assembly operators.
 
 ; The code model option names for -mcmodel.
 Enum
@@ -223,10 +223,6 @@ Target Var(TARGET_DIRECT_EXTERN_ACCESS) Init(0)
 Avoid using the GOT to access external symbols.
 
 mrelax
-Target Var(loongarch_mrelax) Init(HAVE_AS_MRELAX_OPTION && HAVE_AS_COND_BRANCH_RELAXATION)
+Target Var(loongarch_mrelax) Init(HAVE_AS_MRELAX_OPTION)
 Take advantage of linker relaxations to reduce the number of instructions
 required to materialize symbol addresses.
-
-mpass-mrelax-to-as
-Target Var(loongarch_pass_mrelax_to_as) Init(HAVE_AS_MRELAX_OPTION)
-Pass -mrelax or -mno-relax option to the assembler.
diff --git a/src/gcc/config/loongarch/lsx.md b/src/gcc/config/loongarch/lsx.md
index fb4d228ba..075f6ba56 100644
--- a/src/gcc/config/loongarch/lsx.md
+++ b/src/gcc/config/loongarch/lsx.md
@@ -176,6 +176,8 @@
   UNSPEC_LSX_VSSRARNI
   UNSPEC_LSX_VSSRARNI2
   UNSPEC_LSX_VPERMI
+  UNSPEC_LSX_VILVL_INTERNAL
+  UNSPEC_LSX_VREPLVEI_MIRROR
 ])
 
 ;; This attribute gives suffix for integers in VHMODE.
@@ -1551,6 +1553,18 @@
   [(set_attr "type" "simd_flog2")
    (set_attr "mode" "<MODE>")])
 
+;; Only for loongarch_expand_vector_init in loongarch.cc.
+;; Merge two scalar floating-point op1 and op2 into a LSX op0.
+(define_insn "lsx_vilvl_<lsxfmt_f>_internal"
+  [(set (match_operand:FLSX 0 "register_operand" "=f")
+	(unspec:FLSX [(match_operand:<UNITMODE> 1 "register_operand" "f")
+		      (match_operand:<UNITMODE> 2 "register_operand" "f")]
+		     UNSPEC_LSX_VILVL_INTERNAL))]
+  "ISA_HAS_LSX"
+  "vilvl.<lsxfmt>\t%w0,%w2,%w1"
+  [(set_attr "type" "simd_permute")
+   (set_attr "mode" "<MODE>")])
+
 (define_insn "smax<mode>3"
   [(set (match_operand:FLSX 0 "register_operand" "=f")
 	(smax:FLSX (match_operand:FLSX 1 "register_operand" "f")
@@ -2289,6 +2303,16 @@
   [(set_attr "type" "simd_splat")
    (set_attr "mode" "<MODE>")])
 
+(define_insn "lsx_vreplvei_mirror_<lsxfmt_f>"
+  [(set (match_operand:LSX 0 "register_operand" "=f")
+	(unspec: LSX [(match_operand:LSX 1 "register_operand" "f")
+				(match_operand 2 "const_<indeximm>_operand" "")]
+				UNSPEC_LSX_VREPLVEI_MIRROR))]
+  "ISA_HAS_LSX"
+  "vreplvei.d\t%w0,%w1,%2"
+  [(set_attr "type" "simd_splat")
+   (set_attr "mode" "<MODE>")])
+
 (define_insn "lsx_vreplvei_<lsxfmt_f>"
   [(set (match_operand:LSX 0 "register_operand" "=f")
 	(vec_duplicate:LSX
@@ -2450,6 +2474,99 @@
   DONE;
 })
 
+;; Implement vec_concatv2df by vilvl.d.
+(define_insn_and_split "vec_concatv2df"
+  [(set (match_operand:V2DF 0 "register_operand" "=f")
+	(vec_concat:V2DF
+	  (match_operand:DF 1 "register_operand" "f")
+	  (match_operand:DF 2 "register_operand" "f")))]
+  "ISA_HAS_LSX"
+  ""
+  "&& reload_completed"
+  [(const_int 0)]
+{
+  emit_insn (gen_lsx_vilvl_d_f (operands[0],
+				gen_rtx_REG (V2DFmode, REGNO (operands[1])),
+				gen_rtx_REG (V2DFmode, REGNO (operands[2]))));
+  DONE;
+}
+  [(set_attr "mode" "V2DF")])
+
+;; Implement vec_concatv4sf.
+;; Optimize based on hardware register allocation of operands.
+(define_insn_and_split "vec_concatv4sf"
+  [(set (match_operand:V4SF 0 "register_operand" "=f")
+	(vec_concat:V4SF
+	  (vec_concat:V2SF
+	    (match_operand:SF 1 "register_operand" "f")
+	    (match_operand:SF 2 "register_operand" "f"))
+	  (vec_concat:V2SF
+	    (match_operand:SF 3 "register_operand" "f")
+	    (match_operand:SF 4 "register_operand" "f"))))]
+  "ISA_HAS_LSX"
+  ""
+  "&& reload_completed"
+  [(const_int 0)]
+{
+  operands[5] = GEN_INT (1);
+  operands[6] = GEN_INT (2);
+  operands[7] = GEN_INT (4);
+  operands[8] = GEN_INT (8);
+
+  /* If all input are same, use vreplvei.w to broadcast.  */
+  if (REGNO (operands[1]) == REGNO (operands[2])
+      && REGNO (operands[1]) == REGNO (operands[3])
+      && REGNO (operands[1]) == REGNO (operands[4]))
+    {
+      emit_insn (gen_lsx_vreplvei_w_f_scalar (operands[0], operands[1]));
+    }
+  /* If op0 is equal to op3, use vreplvei.w to set each element of op0 as op3.
+     If other input is different from op3, use vextrins.w to insert.  */
+  else if (REGNO (operands[0]) == REGNO (operands[3]))
+    {
+      emit_insn (gen_lsx_vreplvei_w_f_scalar (operands[0], operands[3]));
+      if (REGNO (operands[1]) != REGNO (operands[3]))
+	emit_insn (gen_lsx_vextrins_w_f_scalar (operands[0], operands[1],
+						operands[0], operands[5]));
+      if (REGNO (operands[2]) != REGNO (operands[3]))
+	emit_insn (gen_lsx_vextrins_w_f_scalar (operands[0], operands[2],
+						operands[0], operands[6]));
+      if (REGNO (operands[4]) != REGNO (operands[3]))
+	emit_insn (gen_lsx_vextrins_w_f_scalar (operands[0], operands[4],
+						operands[0], operands[8]));
+    }
+  /* If op0 is equal to op4, use vreplvei.w to set each element of op0 as op4.
+     If other input is different from op4, use vextrins.w to insert.  */
+  else if (REGNO (operands[0]) == REGNO (operands[4]))
+    {
+      emit_insn (gen_lsx_vreplvei_w_f_scalar (operands[0], operands[4]));
+      if (REGNO (operands[1]) != REGNO (operands[4]))
+	emit_insn (gen_lsx_vextrins_w_f_scalar (operands[0], operands[1],
+						operands[0], operands[5]));
+      if (REGNO (operands[2]) != REGNO (operands[4]))
+	emit_insn (gen_lsx_vextrins_w_f_scalar (operands[0], operands[2],
+						operands[0], operands[6]));
+      if (REGNO (operands[3]) != REGNO (operands[4]))
+	emit_insn (gen_lsx_vextrins_w_f_scalar (operands[0], operands[3],
+						operands[0], operands[7]));
+    }
+  /* Otherwise, use vilvl.w to merge op1 and op2 first.
+     If op3 is different from op1, use vextrins.w to insert.
+     If op4 is different from op2, use vextrins.w to insert.  */
+  else
+    {
+      emit_insn (
+	gen_lsx_vilvl_w_f (operands[0],
+			   gen_rtx_REG (V4SFmode, REGNO (operands[1])),
+			   gen_rtx_REG (V4SFmode, REGNO (operands[2]))));
+      emit_insn (gen_lsx_vextrins_w_f_scalar (operands[0], operands[3],
+					      operands[0], operands[7]));
+      emit_insn (gen_lsx_vextrins_w_f_scalar (operands[0], operands[4],
+					      operands[0], operands[8]));
+    }
+  DONE;
+}
+  [(set_attr "mode" "V4SF")])
 
 (define_insn "vandn<mode>3"
   [(set (match_operand:LSX 0 "register_operand" "=f")
@@ -4465,3 +4582,20 @@
   "vpermi.w\t%w0,%w2,%3"
   [(set_attr "type" "simd_bit")
    (set_attr "mode" "V4SI")])
+
+;; Delete one of two instructions that exactly play the same role.
+(define_peephole2
+  [(set (match_operand:V2DI 0 "register_operand")
+	(vec_duplicate:V2DI (match_operand:DI 1 "register_operand")))
+   (set (match_operand:V2DI 2 "register_operand")
+	(vec_merge:V2DI
+	  (vec_duplicate:V2DI (match_operand:DI 3 "register_operand"))
+	  (match_operand:V2DI 4 "register_operand")
+	  (match_operand 5 "const_int_operand")))]
+  "operands[0] == operands[2] &&
+   operands[1] == operands[3] &&
+   operands[2] == operands[4] &&
+   INTVAL (operands[5]) == 2"
+  [(set (match_dup 0)
+	(vec_duplicate:V2DI (match_dup 1)))]
+  "")
diff --git a/src/gcc/config/loongarch/sync.md b/src/gcc/config/loongarch/sync.md
index 483f75de6..dd1f98946 100644
--- a/src/gcc/config/loongarch/sync.md
+++ b/src/gcc/config/loongarch/sync.md
@@ -162,42 +162,18 @@
    (clobber (match_scratch:GPR 6 "=&r"))]
   ""
 {
-  output_asm_insn ("1:", operands);
-  output_asm_insn ("ll.<amo>\t%0,%1", operands);
-
-  /* Like the test case atomic-cas-int.C, in loongarch64, O1 and higher, the
-     return value of the val_without_const_folding will not be truncated and
-     will be passed directly to the function compare_exchange_strong.
-     However, the instruction 'bne' does not distinguish between 32-bit and
-     64-bit operations.  so if the upper 32 bits of the register are not
-     extended by the 32nd bit symbol, then the comparison may not be valid
-     here.  This will affect the result of the operation.  */
-
-  if (TARGET_64BIT && REG_P (operands[2])
-      && GET_MODE (operands[2]) == SImode)
-    {
-      output_asm_insn ("addi.w\t%6,%2,0", operands);
-      output_asm_insn ("bne\t%0,%6,2f", operands);
-    }
-  else
-    output_asm_insn ("bne\t%0,%z2,2f", operands);
-
-  output_asm_insn ("or%i3\t%6,$zero,%3", operands);
-  output_asm_insn ("sc.<amo>\t%6,%1", operands);
-  output_asm_insn ("beqz\t%6,1b", operands);
-  output_asm_insn ("b\t3f", operands);
-  output_asm_insn ("2:", operands);
-  output_asm_insn ("%G5", operands);
-  output_asm_insn ("3:", operands);
-
-  return "";
+  return "1:\\n\\t"
+	 "ll.<amo>\\t%0,%1\\n\\t"
+	 "bne\\t%0,%z2,2f\\n\\t"
+	 "or%i3\\t%6,$zero,%3\\n\\t"
+	 "sc.<amo>\\t%6,%1\\n\\t"
+	 "beqz\\t%6,1b\\n\\t"
+	 "b\\t3f\\n\\t"
+	 "2:\\n\\t"
+	 "%G5\\n\\t"
+	 "3:\\n\\t";
 }
-  [(set (attr "length")
-     (if_then_else
-	(and (match_test "GET_MODE (operands[2]) == SImode")
-	     (match_test "REG_P (operands[2])"))
-	(const_int 32)
-	(const_int 28)))])
+  [(set (attr "length") (const_int 28))])
 
 (define_expand "atomic_compare_and_swap<mode>"
   [(match_operand:SI 0 "register_operand" "")   ;; bool output
-- 
2.45.2

